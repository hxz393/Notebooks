# 应用升级

## 手动更新pod内应用

假设pod使用RC或RS控制器来创建和管理,客户端通过Service来访问pod,要对pod内的镜像进行升级有以下方式:

- 直接删除所有现有pod然后创建新的pod.

  当使用RC控制器时,通过将RC内的pod模板镜像修改,然后删除旧的pod实例,RC会采用修改后的镜像来创建新的实例.

  这种方式会导致应用在一定的时间内不可用.

- 先创建新的pod,成功运行后再删除旧的pod.

  同样先修改RC内的pod模板镜像,创建新版本pod且正常运行后,可以修改服务的标签选择器(使用命令kubectl set selector),将流量切换到新的pod,最后再删除旧版本的pod.这也叫蓝绿部署,适用于不能中断服务的场景.

  这种方法需要应用程序支持两个版本的同时对外提供服务.并且新版应用不应对原有的关键数据格式或数据本身进行修改,从而导致之前版本程序运行异常.

- 执行滚动升级操作

  还可以执行滚动升级操作来逐步替代原有的pod,而不是同时创建新pod和同时删除旧pod.具体逐步操作旧版本的RC控制器进行缩容,并对新版本RC控制器进行扩容来拉大新旧版本pod数量比率,这就是金丝雀发布.而服务的选择器同时包含两个版本版本的pod.

  手动执行滚动升级操作非常容易出错.



## 使用RC自动滚动升级

新建一个RC控制器和服务,使用v1版本镜像:

```sh
[root@server4-master ~]# vi kubia-v1.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia-v1
spec:
  replicas: 3
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: NodePort
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30002
[root@server4-master ~]# kubectl create -f kubia-v1.yaml
replicationcontroller/kubia-v1 created
service/kubia created
```

发布成功后3个v1 pod和服务都会开始工作.通过浏览器30002端口访问,并保持输出:

```sh
[root@server4-master ~]# while true; do curl http://192.168.2.204:30002; sleep 1; done
This is v1 running in pod kubia-v1-9ql94
This is v1 running in pod kubia-v1-52pvf
You've hit kubia-k2l4p
You've hit kubia-k2l4p
This is v1 running in pod kubia-v1-52pvf
You've hit kubia-p9wp5
```

使用kubectl rolling-update命令(已废弃)来直接替换掉image镜像,进行滚动更新:

```sh
[root@k8s-master 4]# kubectl rolling-update kubia-v1 kubia-v2 --image=luksa/kubia:v2
Command "rolling-update" is deprecated, use "rollout" instead
Created kubia-v2
Scaling up kubia-v2 from 0 to 3, scaling down kubia-v1 from 3 to 0 (keep 3 pods available, don't exceed 4 pods)
Scaling kubia-v2 up to 1
Scaling kubia-v1 down to 2
Scaling kubia-v2 up to 2
Scaling kubia-v1 down to 1
Scaling kubia-v2 up to 3
Scaling kubia-v1 down to 0
Update succeeded. Deleting kubia-v1
replicationcontroller/kubia-v2 rolling updated to "kubia-v2"
```

开发过程中经常会推送修改后的应用到同一个镜像tag,这会导致镜像不被重新拉取.或者在没有拉取过旧版镜像的节点上,拉取的是新镜像,因此最后可能有两个不同版本pod同时运行.

为了确保始终使用最新版镜像,可以将容器的imagePullPolicy属性设置为Always.如果不指定镜像tag,也就是默认使用latest的tag,则策略默认为Always,但如果指定了其他标记,则策略默认为IfNotPresent.

自动滚动升级的步骤如下:

- kubectl通过复制RC控制器kubia-v1命名为kubia-v2,在RC控制器kubia-v2模板中中改变镜像版本为kubia:v2.
- RC控制器kubia-v2被立即创建,初始期望副本数设置为0.
- 修改RC控制器kubia-v1和kubia-v2的标签选择器,添加一个额外的deployment标签.
- 修改旧版本pod的标签,添加一个额外的deployment标签.
- 以1为单位,RC控制器kubia-v1开始缩减pod数量,同时RC控制器kubia-v1开始扩展pod数量,新pod也带有一个额外的deployment标签.
- 最终旧版pod数量被收缩到0,全部被新版本pod所替代,旧RC控制器kubia-v1被删除.

以这种形式滚动升级,所有伸缩请求都是由kubectl客户端而不是主节点执行,一旦kubectl执行升级时失去连接,升级会被中断,pod和RC最终会处于中间状态.一个更先进的方式是以Deployment资源来部署.

