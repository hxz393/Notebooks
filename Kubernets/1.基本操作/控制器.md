# 控制器

## ReplicationController

下面简称RC.可以通过kubectl api-resources命令查看所有缩写.

### 创建RC

创建一个最简化运行3个pod副本的RC配置如下:

```sh
[root@server4-master ~]# vi kubia-rc.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
[root@server4-master ~]# kubectl create -f kubia-rc.yaml 
replicationcontroller/kubia created
```

标签选择器需要与pod的labels标签匹配,否则RC会一直启动新容器.假如不指定选择器selector,RC会根据pod模板中的标签自动配置.

运行后K8s会创建一个名为kubia的新RC,并且始终运行3个实例.没有足够pod时会根据模板创建新pod.下面手动删除一个pod,RC会立即重建一个新的容器:

```sh
[root@server4-master ~]# kubectl delete po kubia-wtlrp
pod "kubia-wtlrp" deleted
[root@server4-master ~]# kubectl get po
NAME          READY   STATUS        RESTARTS   AGE
kubia-2vpxd   1/1     Running       0          105s
kubia-9fqql   1/1     Running       0          105s
kubia-slltr   1/1     Running       0          22s
kubia-wtlrp   1/1     Terminating   0          105s
```

如果有节点故障,那么RC会在新的节点上启动节点故障上的pod,之后节点故障恢复也不会再迁移回去.

### 查看RC

查询当前运行的所有RC:

```sh
[root@server4-master ~]# kubectl get rc
NAME    DESIRED   CURRENT   READY   AGE
kubia   3         3         3       9m37s
```

查询具体RC的信息同样使用kubectl describe命令:

```sh
[root@server4-master ~]# kubectl describe rc kubia 
Name:         kubia
Namespace:    default
Selector:     app=kubia
Labels:       app=kubia
Annotations:  <none>
Replicas:     3 current / 3 desired
P Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
```

### 修改模板

可以通过kubectl edit rc命令来修改RC的pod模板,例如修改模板中的标签选择器和副本数:

```sh
[root@server4-master ~]# kubectl edit rc kubia
spec:
  replicas: 2
  selector:
    app: kubia1
  template:
    metadata:
      labels:
        app: kubia1
"/tmp/kubectl-edit-2608748675.yaml" 46L, 1157C written
replicationcontroller/kubia edited
```

查看pod和Label发现共有5个pod,新旧标签一同存在:

```sh
[root@server4-master ~]# kubectl get po --show-labels 
NAME          READY   STATUS    RESTARTS   AGE   LABELS
kubia-2c44j   1/1     Running   0          56s   app=kubia1
kubia-2vpxd   1/1     Running   0          40m   app=kubia
kubia-9fqql   1/1     Running   0          40m   app=kubia
kubia-ggk2j   1/1     Running   0          56s   app=kubia1
kubia-slltr   1/1     Running   0          38m   app=kubia
```

### 水平缩放

使用scale命令能修改RC配置中spec.replicas字段的数值,达到扩缩容效果:

```sh
[root@server4-master ~]# kubectl scale rc kubia --replicas=1
replicationcontroller/kubia scaled
[root@server4-master ~]# kubectl get pod --show-labels
NAME          READY   STATUS        RESTARTS   AGE     LABELS
kubia-2c44j   1/1     Running       0          5m18s   app=kubia1
kubia-ggk2j   1/1     Terminating   0          5m18s   app=kubia1
```

### 删除RC

当使用delete删除RC时,pod也会被删除.可以指定--cascade=orphan选项来删除RC同时保持pod运行:

```sh
[root@server4-master ~]# kubectl delete rc kubia --cascade=orphan
replicationcontroller "kubia" deleted
```

之后可以通过标签选择器创建新的RC或RS将它们再次管理起来.



## ReplicaSet

下面简称RS.RS和RC都是依赖pod标签选择器来控制.

### 创建RS

建立yaml配置文件并发布.和RC配置不同的是apiVersion版本, kind类型和标签选择器样式:

```sh
[root@server4-master ~]# vi kubia-rs.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - name: http
          containerPort: 80
[root@server4-master ~]# kubectl create -f kubia-rs.yaml
replicaset.apps/kubia created
```

### 标签选择

标签选择器和pod中使用的一样,不过写法有些不同.例如通过matchLabels匹配单个标签:

```sh
[root@server4-master ~]# vi kubia-rs.yaml
spec:
  selector:
    matchLabels:
      app: kubia
```

例如通过matchExpressions表达式同时选择两个不同app值的标签:

```sh
[root@server4-master ~]# vi kubia-rs.yaml
spec:
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - kubia
          - kubia1
```

operator还可以使用其他运算符: NotIn(不在列表中),Exists(匹配Key),DoesNotExist(反向匹配Key)

如果指定了多个表达式,所有表达式匹配结果都必须为true才能使选择器与pod匹配.

如果同时使用matchLabels和matchExpressions,则所有标签都必须匹配.



## DaemonSet

下面简称DS.DS和RS相比多了一个nodeSelector选择器,DS依赖节点标签选择器来控制.

### 创建DS

首先给一个节点打上标签disk='ssd':

```sh
[root@server4-master ~]# kubectl label node server5-node1 disk='ssd'
node/server5-node1 labeled
```

然后建立YAML配置文件并启动:

```sh
[root@server4-master ~]# vi kubia-ds.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: ds-ssd
spec:
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      nodeSelector:
        disk: ssd
      containers:
      - name: kubia
        image: luksa/kubia
[root@server4-master ~]# kubectl create -f kubia-ds.yaml 
daemonset.apps/ds-ssd created
```

DS还可以配置更新机制,相关配置定义在spec.update-Strategy字段,方式为RollingUpdate(滚动更新)或OnDelete(删除再更新).回滚操作同样支持.

### 验证

将server6-node2也打上ssd标签后再查看信息:

```sh
[root@server4-master ~]# kubectl label node server6-node2 disk='ssd'
[root@server4-master ~]# kubectl get pod -o wide
NAME           READY   STATUS    RESTARTS   AGE     IP               NODE     
ds-ssd-drb2z   1/1     Running   0          4m19s   10.244.191.199   server5-node1  
ds-ssd-f6cwh   1/1     Running   0          23s     10.244.244.199   server6-node2  
```

去除节点标签后运行在其上的pod会立即删除:

```sh
[root@server4-master ~]# kubectl label node server6-node2 disk-
[root@server4-master ~]# kubectl label node server5-node1 disk-
[root@server4-master ~]# kubectl get ds
NAME     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
ds-ssd   0         0         0       0            0           disk=ssd        7m52s
```



## Job

Job使用api为batch/v1.需要明确地将重启策略设置为OnFailure或Nerver,防止容器在完成任务时重新启动.

### 创建Job

这里调用了一个运行120秒的进程然后退出.指定restartPolicy属性为OnFailure(默认为Always):

```sh
[root@server4-master ~]# vi kubia-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: OnFailure
      containers:
      - name: main
        image: luksa/batch-job
[root@server4-master ~]# kubectl create -f kubia-job.yaml
```

当Job任务完成后状态显示为Completed:

```sh
[root@server4-master ~]# kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           119s       2m59s
[root@server4-master ~]# kubectl get pod
NAME                 READY   STATUS      RESTARTS   AGE
batch-job--1-2ddkr   0/1     Completed   0          3m10s
```

出于查询日志的需求,完成任务的pod不会自动删除,可以通过删除创建的Job来一同删除.

### 多次运行

可以在Job中配置创建多个pod实例,设置completions和parallelism属性来以并行或串行方式运行.

顺序运行用于一个Job运行多次的场景,例如设置顺序运行五个pod,每个pod成功完成后工作才结束:

```sh
[root@server4-master ~]# vi kubia-job.yaml
spec:
  completions: 5
[root@server4-master ~]# kubectl create -f kubia-job.yaml
[root@server4-master ~]# kubectl get jobs
NAME        COMPLETIONS   DURATION   AGE
batch-job   0/5           12s        12s
```

设置并行运行需要多加一个parallelism参数设置并行启动pod数目:

```sh
[root@server4-master ~]# vi kubia-job.yaml
spec:
  completions: 5
  parallelism: 3
[root@server4-master ~]# kubectl delete job batch-job 
[root@server4-master ~]# kubectl create -f kubia-job.yaml
[root@server4-master ~]# kubectl get po
NAME                 READY   STATUS    RESTARTS   AGE
batch-job--1-5ghsj   1/1     Running   0          39s
batch-job--1-mctcz   1/1     Running   0          39s
batch-job--1-znpbq   1/1     Running   0          39s
```

Job的并行任务数同样可以通过scale命令来修改

### 限制条件

可以通过activeDeadlineSeconds属性来限制pod的运行时间.如果超时没完成,系统会尝试终止pod并标记失败.

还能配置Job被标记为失败前重试的次数,通过spec.backoffLimit字段,默认为6次.



## CronJob

下面简称CJ.K8s通过CJ中配置的Job模板创建Job资源.

### 创建CJ

创建一个每十五分钟运行一次的Job:

```sh
[root@server4-master ~]# vi kubia-cj.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: 15job
spec:
  schedule: "0,15,30,45 * * * *"
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: 15job
        spec:
          restartPolicy: OnFailure
          containers:
          - name: main
            image: luksa/batch-job
[root@server4-master ~]# kubectl create -f kubia-cj.yaml 
[root@server4-master ~]# kubectl get cj
NAME    SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE
15job   0,15,30,45 * * * *   False     0        <none>          16s
```

其中schedule段五个设置分别是: `分钟 小时 每月日期 月 星期`

### 超时设置

可以通过startingDeadlineSeconds字段来设置预定运行超时时间,例如不能超过预定时间15秒后运行:

```sh
[root@server4-master ~]# vi kubia-cj.yaml
spec:
  startingDeadlineSeconds: 15
```

不管任何原因,时间超过了15秒而没启动任务,任务将不会运行并显示失败.

### 其他设置

其他一些spec字段可嵌套使用字段:

- concurrencyPolicy: 并发执行策略,用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业.可选值Allow, Forbid或Replace.

- failedJobHistoryLimit: 为失败的任务执行保留的历史记录数,默认1.

- successfulJobsHistoryLimit: 为成功的任务执行保留的历史记录数,默认3.

- startingDeadlineSeconds: 设置超时时间,超时没完成任务会被记入错误历史记录.

- suspend: 是否挂起后续的任务执行,默认false,对运行中的作业不会产生影响.



## Deployment

Deployment是一种更高阶资源,用于部署应用程序并以声明的方式升级应用.创建一个Deployment资源时,RS资源也会随之创建,pod实际由RS创建和管理.Deployment主要负责处理升级应用时两个版本的控制器之间关系.

### 创建Deploy

创建一个Deploy与创建RC的声明大体一致,只是Deploy声明包含额外部署策略字段.因为Deploy可以同时管理多个版本的pod,所以不需要给Deploy命名中加入版本号:

```sh
[root@server4-master ~]# vi kubia-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v1
        name: nodejs
---
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: NodePort
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30002
```

创建Deploy时加入--record选项,可以记录CHANGE-CAUSE信息,也就是声明文件中annotation字段的注解:

```sh
[root@server4-master ~]# kubectl create -f kubia-deploy.yaml --record
Flag --record has been deprecated, --record will be removed in the future
deployment.apps/kubia created
service/kubia created
```

命令kubectl rollout专门用于查询Deploy部署状态:

```sh
[root@server4-master ~]# kubectl rollout status deployment kubia
deployment "kubia" successfully rolled out
[root@server4-master ~]# kubectl get all
NAME                         READY   STATUS        RESTARTS   AGE
pod/kubia-74967b5695-5s68q   1/1     Running       0          10s
pod/kubia-74967b5695-8cxtb   1/1     Running       0          10s
pod/kubia-74967b5695-bwzb9   1/1     Running       0          10s
pod/kubia-74967b5695-jkcjl   0/1     Terminating   0          2m55s

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        133d
service/kubia        NodePort    10.101.57.66   <none>        80:30002/TCP   10s

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/kubia   3/3     3            3           10s

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/kubia-74967b5695   3         3         3       10s
```

新建的pod名字中间多了一串数字,这是pod模板的Hash值.由Deploy创建的RS控制器也带有同样的哈希值.这样Deploy能用来对应和管理一个版本的pod模板.

通过30002端口访问服务来测试pod工作状态:

```sh
[root@server4-master ~]# curl localhost:30002
This is v1 running in pod kubia-74967b5695-5s68q
```

### 升级Deploy

Deploy默认的升级策略是执行滚动更新(RollingUpdate).另一个策略为Recreate,它会一次性删除所有旧版本pod,然后创建新pod.

通过patch命令修改单个或少量资源属性非常有用,但更改Deploy的自有属性不会触发pod的任何更新.这里设置一个时间来减慢滚动更新的速度:

```sh
[root@server4-master ~]# kubectl patch deploy kubia -p '{"spec": {"minReadySeconds": 10}}'
deployment.apps/kubia patched
```

采用set image命令来更改任何包含容器资源中的镜像:

```sh
[root@server4-master ~]# kubectl set image deployment kubia nodejs=luksa/kubia:v2
deployment.apps/kubia image updated
```

通过describe查询可以看到Deploy的方式和RC滚动升级类似,也是先增加新版本pod缩减旧pod,最后完成升级.

```sh
[root@server4-master ~]# kubectl get all
NAME                         READY   STATUS        RESTARTS   AGE
pod/kubia-74967b5695-5s68q   1/1     Running       0          3m40s
pod/kubia-74967b5695-8cxtb   1/1     Terminating   0          3m40s
pod/kubia-74967b5695-bwzb9   1/1     Running       0          3m40s
pod/kubia-bcf9bb974-9kb8x    1/1     Running       0          3s
pod/kubia-bcf9bb974-bqwng    1/1     Running       0          21s
```

整个升级过程由运行在K8s上的一个控制器处理和完成,简单又可靠.

如果Deployment中的pod模板引用了一个ConfigMap(Secret)那么更改ConfigMap资源不会触发升级操作,需要创建一个新的CM并修改pod模板引用新的CM.

### 回滚升级

假如升级的版本有问题,可以自动停止升级.首先升级到有错误的版本,这个应用在第5个请求之后会返回内部服务器错误,也就是HTTP状态码500:

```sh
[root@server4-master ~]# kubectl set image deploy kubia nodejs=luksa/kubia:v3
deployment.apps/kubia image updated
[root@server4-master ~]# kubectl rollout status deploy kubia
Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...
Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...
deployment "kubia" successfully rolled out
```

可使用rollout undo命令回滚到上一版本:

```sh
[root@server4-master ~]# kubectl rollout undo deploy kubia
deployment.apps/kubia rolled back
```

undo命令也能在滚动升级过程中运行,并直接停止升级回归老版本.

最后通过rollout history来显示升级版本历史记录:

```sh
[root@server4-master ~]# kubectl rollout history deployment kubia
deployment.apps/kubia 
REVISION  CHANGE-CAUSE
1         kubectl create --filename=kubia-deploy.yaml --record=true
3         kubectl create --filename=kubia-deploy.yaml --record=true
4         kubectl create --filename=kubia-deploy.yaml --record=true
```

滚动升级成功后,老版本RS不会被删掉,K8s保留着完整修改版本历史.历史记录保留数目默认为2,由Deploy中revisionHistoryLimit的属性值来限制,更早的RS都会被删除.

因此可以通过undo命令指定一个特定版本号,来回滚到指定版本:

```sh
[root@server4-master ~]# kubectl rollout undo deploy kubia --to-revision=1
deployment.apps/kubia rolled back
```

--to-revision的参数对应着history上的版本.如果手动删除遗留RS会导致历史版本记录丢失无法回滚.另外扩容操作不会创建版本.



### 滚动升级策略属性

可以在Deploy配置文件spec.strategy.rollingUpdate下修改两个滚动升级策略的属性:

- maxSurge

  决定了Deploy配置中期望的副本数之外,最多允许超出的pod实例数量.默认为25%.比如期望副本数量为4,那么在滚动升级期间,不会运行超过5个pod实例.把百分数转为绝对值会将数字四舍五入.也可直接指定绝对值.

- maxUnavailable

  决定在滚动升级期间,相对于期望副本数能够允许有多少个pod实例处于不可用状态.默认值25%表示可用pod实例的数量不能低于期望副本数的75%.假如期望副本数为4,那么只能有一个pod处于不可用状态,也可以使用绝对值设定.

假设运行副本数量为3,当设置为maxSurge=1, maxUnavailable=0时,表示需要始终保持3个可用的副本,升级中最多运行4个副本数量.每次启动1个新副本,新副本运行正常后,删掉1个旧副本,直到所有旧副本被替换掉.

extensions/v1beta1版本的Deploy使用不一样的默认值,两个参数都被设置为1,表示允许1个副本不可用状态,升级中最多运行4个副本数量.首先删除1个旧副本,启动2个新副本,新副本运行正常后,继续删除1个旧副本启动1个新副本,直到所有旧副本被替换掉.

### 暂停滚动更新

可以在更新过程中使用rollout pause暂停升级:

```sh
[root@server4-master ~]# kubectl set image deploy kubia nodejs=luksa/kubia:v4
deployment.apps/kubia image updated
[root@server4-master ~]# kubectl rollout pause deploy kubia
deployment.apps/kubia paused
```

现在应用处于1个新版本pod加3个旧版本pod的混合运行状态,服务的一部分请求将被切换到新的pod.通过这种方式只有一部分用户会访问到新版本,相当于运行了一个金丝雀版本.在验证新版本是否正常工作后,可以将剩余pod继续升级或回滚到上一个版本.

可以把两条命令用&&符号连接:

```sh
[root@server4-master ~]# kubectl set image deploy kubia nodejs=luksa/kubia:v1 && kubectl rollout pause deploy kubia
deployment.apps/kubia image updated
deployment.apps/kubia paused
[root@server4-master ~]# kubectl get all
NAME                         READY   STATUS    RESTARTS   AGE
pod/kubia-74967b5695-ckpf9   1/1     Running   0          53s
pod/kubia-7bddb8bfc7-fjkkg   1/1     Running   0          2m
pod/kubia-7bddb8bfc7-hnjh4   1/1     Running   0          2m12s
pod/kubia-7bddb8bfc7-wtt8g   1/1     Running   0          108s

NAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1      <none>        443/TCP        133d
service/kubia        NodePort    10.101.57.66   <none>        80:30002/TCP   37m

NAME                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/kubia   4/3     1            4           37m

NAME                               DESIRED   CURRENT   READY   AGE
replicaset.apps/kubia-555774bf68   0         0         0       9m58s
replicaset.apps/kubia-74967b5695   1         1         1       37m
replicaset.apps/kubia-7bddb8bfc7   3         3         3       31m
replicaset.apps/kubia-bcf9bb974    0         0         0       34m
[root@server4-master ~]# curl localhost:30002
This is v3 running in pod kubia-7bddb8bfc7-wtt8g
[root@server4-master ~]# curl localhost:30002
This is v1 running in pod kubia-74967b5695-ckpf9
```

滚动升级的进度无法控制,因此想要进行金丝雀发布的正确方式是,使用两个不同的Deploy并同时调整它们对应的pod数量.

### 恢复滚动升级

在暂停升级期间,撤销命令不起作用,在恢复升级后才进行撤销操作.使用rollout resume来恢复升级.:

```sh
[root@server4-master ~]# kubectl rollout resume deploy kubia
deployment.apps/kubia resumed
[root@server4-master ~]# kubectl rollout undo deploy kubia --to-revision=3
deployment.apps/kubia rolled back
```

暂停部署还能用来阻止更新Deploy而自动触发的滚动升级过程,可以对Deployment进行多次更改,并在完成所有更改后才恢复滚动升级,一旦更改完毕则恢复更新过程.

### 配置就绪探针

使用minReadySeconds属性设置新pod要成功运行多久后,才将其视为可用.当所有容器的就绪探针返回成功时,pod就标记为就绪状态.如果一个新pod运行出错,并且在minReadySeconds时间内它的就绪探针出现了失败,那么新版本的滚动升级将被阻止.一般会将minReadySeconds设置为更高的值,来确保Pod在真正接收实际流量后可以持续保持就绪状态.

使用之前会报错的v3镜像,来测试就绪探针的作用:

```sh
[root@server4-master ~]# vi kubia-v3.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  minReadySeconds: 10
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - image: luksa/kubia:v3
        name: nodejs
        readinessProbe:
          periodSeconds: 1
          httpGet:
            path: /
            port: 8080
```

设置maxUnavailabel的值为0确保升级过程中pod被挨个替换.就绪探针GET请求每秒执行1次,到第6秒开始报错.使用apply升级Deploy不仅会更新镜像,还添加了就绪探针以及其他参数:

```sh
[root@server4-master ~]# kubectl apply -f kubia-v3.yaml
Warning: resource deployments/kubia is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/kubia configured
```

观察curl输出,可以看到流量并没有转发到v3版本的pod上面,因为新pod未就绪,被从Service的endpoint中移除:

```sh
[root@server4-master ~]# curl localhost:30002
This is v1 running in pod kubia-74967b5695-8kmxn
[root@server4-master ~]# curl localhost:30002
This is v1 running in pod kubia-74967b5695-ckpf9
[root@server4-master ~]# curl localhost:30002
This is v1 running in pod kubia-74967b5695-ls2z2
[root@server4-master ~]# kubectl get po
NAME                     READY   STATUS    RESTARTS   AGE
kubia-67d49c55dd-wcnwd   0/1     Running   0          60s
kubia-74967b5695-8kmxn   1/1     Running   0          4m40s
kubia-74967b5695-ckpf9   1/1     Running   0          10m
kubia-74967b5695-ls2z2   1/1     Running   0          4m28s
[root@server4-master ~]# kubectl describe po kubia-67d49c55dd-wcnwd
Events:
  Type     Reason     Age                     From               Message
  ----     ------     ----                    ----               -------
  Warning  Unhealthy  2m51s (x22 over 3m11s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 500
```

之后升级一直处于进行中状态,因为maxUnavailabel为0,所以既不会创建新pod,也不会删除任何原始pod.如果没有正确设置minReadySeconds,一旦有以此就绪探针调用成功,便会认为新pod已经可用,因此可以把时间设长一点.

默认情况下滚动升级在10分钟内不能完成会被视为失败.这个时限在spec.progressDeadlineSeconds中设定:

```sh
[root@server4-master ~]# kubectl describe deploy kubia
Name:                   kubia
Namespace:              default
Replicas:               3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        10
RollingUpdateStrategy:  0 max unavailable, 1 max surge
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    False   ProgressDeadlineExceeded
OldReplicaSets:  kubia-74967b5695 (3/3 replicas created)
NewReplicaSet:   kubia-67d49c55dd (1/1 replicas created)
```

滚动升级卡住之后,只能通过rollout undo命令来取消滚动升级.



## StatefulSet

StatefulSet可以看做是Deployment的一个特殊变种,有如下特性:

- StatefulSet里每个pod都有稳定,唯一的网络标识,用来发现集群内其他成员,假设StatefulSet的名字叫kafka,那么第一个pod叫kafka-0,第二个叫kafka-1.

- StatefulSet控制的pod副本启动顺序是受控的,操作第n个pod时,前一个pod已经时运行且准备好的状态.

- StatefulSet里的pod采用稳定的持久化储存卷(PV),删除pod时默认不会删除相关储存卷.

完整可用的StatefulSet通常有三个组件构成:StatefulSet,headless Service和volumeClaimTemplate.

### 应用状态

按照应用状态分,通常四种应用程序类型:

- 具有读写磁盘需求的有状态应用程序.如各种RDBMS储存系统,分布式储存系统Redis Cluster, MongoDB, ZooKeeper, Cassandra等.
- 包含两类应用程序,一类是那些具有读写磁盘需求的无状态应用程序,另一类是仅需读取权限的无状态应用比如Web服务程序.
- 无磁盘访问需求的无状态应用程序
- 无磁盘访问需求的有状态应用程序,比如说淘宝的购物车系统.

### 有状态服务

k8s中pod的管理对象RC, RS, Deploy, DS和Job都是面向无状态的服务,但有很多服务是有状态的,例如MySQL集群, MongoDB集群, Akka集群等.这些集群有一些共同点:

- 每个节点都有固定的身份ID,通过ID互相发现和通信;

- 集群的规模比较固定,不能随意变动;

- 集群里的每个节点都是有状态的,通常会持久化数据到永久储存中;

- 如果磁盘损坏,则集群里的某个节点无法正常运行,集群功能受损.

### 稳定的网络标识

由StatefulSet创建的每个pod都有一个从0开始的顺序索引,体现到pod的名称,主机名和固定储存上.

StatefulSet通常与headless Service配合使用.如果解析headless Service的DNS域名,返回Service对应的全部pod的Endpoint列表.StatefulSet在headless Service的基础上又为StatefulSet控制的每个Pod实例创建了一个DNS域名,格式为:`$(podname).$​(headless service name).$(namespace).svc.cluster.local.`

当一个StatefulSet管理的pod实例消失后,StatefulSet会保证重启一个新的拥有与之前pod完全一致名称和主机名的pod实例替换它.

扩容一个StatefulSet会使用下一个还没用到的顺序索引值创建一个新的pod实例.缩容时会先删除最高索引值的实例,每次只会操作一个pod实例,并且在有实例不健康的情况下不允许做缩容操作.

### 稳定的专属储存

有状态的pod的储存必须是持久的,并与pod解耦.StatefulSet需要定义一个或多个卷声明模板,绑定到pod实例上.

在扩容StatefulSet时,会创建新pod和与之关联的一个或多个持久卷声明.缩容时则只会删除pod保留持久卷声明,在重新扩容后,新的pod会与先前的持久卷绑定.

### 创建STS

创建一个简单的应用接受POST请求,把请求中body数据写入/var/data/kubia.txt,在收到GET请求时,返回主机名和储存数据:

```sh
root@ec837c407906:/# cat app.js
const http = require('http');
const os = require('os');
const fs = require('fs');

const dataFile = "/var/data/kubia.txt";

function fileExists(file) {
  try {
    fs.statSync(file);
    return true;
  } catch (e) {
    return false;
  }
}

var handler = function(request, response) {
  if (request.method == 'POST') {
    var file = fs.createWriteStream(dataFile);
    file.on('open', function (fd) {
      request.pipe(file);
      console.log("New data has been received and stored.");
      response.writeHead(200);
      response.end("Data stored on pod " + os.hostname() + "\n");
    });
  } else {
    var data = fileExists(dataFile) ? fs.readFileSync(dataFile, 'utf8') : "No data posted yet";
    response.writeHead(200);
    response.write("You've hit " + os.hostname() + "\n");
    response.end("Data stored on this pod: " + data + "\n");
  }
};

var www = http.createServer(handler);
www.listen(8080);
```

创建基于NFS的持久化储存卷:

```sh
[root@server4-master ~]# cd /srv/pv/
[root@server4-master pv]# mkdir pv001 pv002 pv003
[root@server4-master pv]# echo "/srv/pv/pv001 *(rw,no_root_squash,sync)" >> /etc/exports
[root@server4-master pv]# echo "/srv/pv/pv002 *(rw,no_root_squash,sync)" >> /etc/exports
[root@server4-master pv]# echo "/srv/pv/pv003 *(rw,no_root_squash,sync)" >> /etc/exports
[root@server4-master pv]# exportfs -a
[root@server4-master pv]# showmount -e
Export list for server4-master:
/srv/pv/pv003 *
/srv/pv/pv002 *
/srv/pv/pv001 *
/srv/pv       *
[root@server4-master pv]# exportfs -v
/srv/pv         <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
/srv/pv/pv001   <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
/srv/pv/pv002   <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
/srv/pv/pv003   <world>(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,no_root_squash,no_all_squash)
```

可以使用List对象来定义一组pv资源和使用--分隔效果一样.设定storageClassName为nfs:

```sh
[root@server4-master ~]# vi pv-list.yaml
kind: List
apiVersion: v1
items:
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv001
  spec:
    capacity:
      storage: 10Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Retain
    storageClassName: nfs
    nfs:
      path: /srv/pv/pv001
      server: server4-master
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv002
  spec:
    capacity:
      storage: 10Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Retain
    storageClassName: nfs
    nfs:
      path: /srv/pv/pv002
      server: server4-master
- apiVersion: v1
  kind: PersistentVolume
  metadata:
    name: pv003
  spec:
    capacity:
      storage: 10Mi
    accessModes:
      - ReadWriteOnce
    persistentVolumeReclaimPolicy: Retain
    storageClassName: nfs
    nfs:
      path: /srv/pv/pv003
      server: server4-master
[root@server4-master ~]# kubectl create -f pv-list.yaml 
persistentvolume/pv001 created
persistentvolume/pv002 created
persistentvolume/pv003 created
[root@server4-master ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                 STORAGECLASS   REASON   AGE
pv001        10Mi       RWO            Retain           Available                         nfs                     8s
pv002        10Mi       RWO            Retain           Available                         nfs                     8s
pv003        10Mi       RWO            Retain           Available                         nfs                     8s
```

在部署StatefulSet之前,还需要创建一个用于在有状态的pod之间提供网络标识的headless Service,用于pod之间互相发现:

```sh
[root@server4-master ~]# vi kubia-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  clusterIP: None
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
[root@server4-master ~]# kubectl create -f kubia-svc-headless.yaml
service/kubia created
```

最后是StatefulSet配置,在spec中必须包含serviceName和template字段:

```sh
[root@server4-master ~]# vi kubia-st.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kubia
spec:
  serviceName: kubia
  replicas: 2
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia-pet
        ports:
        - name: http
          containerPort: 8080
        volumeMounts:
        - mountPath: "/var/data"
          name: data
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Mi
      storageClassName: nfs
```

通过一个名为data的PVC模板来为每个pod创建一个持久卷声明,其中要指定storageClassName: nfs来选择前面创建的PV,这样PVC会自动关联到可使用的PV上:

```sh
[root@server4-master ~]# kubectl create -f kubia-st.yaml
statefulset.apps/kubia created
[root@server4-master ~]# kubectl get all
NAME          READY   STATUS              RESTARTS   AGE
pod/kubia-0   1/1     Running             0          13s
pod/kubia-1   0/1     ContainerCreating   0          7s
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   135d
service/kubia        ClusterIP   None         <none>        80/TCP    29m
NAME                     READY   AGE
statefulset.apps/kubia   1/2     13s
[root@server4-master ~]# kubectl get pvc
NAME           STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
data-kubia-0   Bound    pv001        10Mi       RWO            nfs            42s
data-kubia-1   Bound    pv002        10Mi       RWO            nfs            36s
[root@server4-master ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                  STORAGECLASS   REASON   AGE
pv001        10Mi       RWO            Retain           Bound       default/data-kubia-0   nfs                     64s
pv002        10Mi       RWO            Retain           Bound       default/data-kubia-1   nfs                     64s
pv003        10Mi       RWO            Retain           Available                          nfs                     64s
```

第二个pod会在第一个pod运行并且处于就绪状态后创建.全部就绪后可以看到有2个PV已经被新创建的pod所绑定.

### 测试STS

因为服务为headless模式,所以不能通过服务来访问服务.可以先在节点上运行代理,再通过curl来通过API服务器与pod通信:

```sh
[root@server4-master ~]# kubectl proxy
Starting to serve on 127.0.0.1:8001
[root@server4-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: No data posted yet
```

使用POST请求发送数据后,再通过GET请求查询:

```sh
[root@server4-master ~]# curl -X POST -d "kubia-0" localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
Data stored on pod kubia-0
[root@server4-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: kubia-0
[root@server4-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-1/proxy/
You've hit kubia-1
Data stored on this pod: No data posted yet
```

手动删除pod后验证重新调度的pod是否关联了相同的储存:

```sh
[root@server4-master ~]# kubectl delete po kubia-0
pod "kubia-0" deleted
[root@server4-master ~]# kubectl get po -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP               NODE            NOMINATED NODE   READINESS GATES
kubia-0   1/1     Running   0          18s     10.244.244.234   server6-node2   <none>           <none>
kubia-1   1/1     Running   0          8m39s   10.244.244.233   server6-node2   <none>           <none>
```

删除后等待一段时间,kubia-0在另一个节点重建好了,用curl试试看数据是否还在:

```sh
[root@server4-master ~]# curl localhost:8001/api/v1/namespaces/default/pods/kubia-0/proxy/
You've hit kubia-0
Data stored on this pod: kubia-0
```

缩容一个StatefulSet只会删除对应的pod,持久卷声明被卸载但保留.

### 发现节点

集群中伙伴节点能彼此发现是很重要的需求,这样才能找到集群中其他成员.虽然可以通过API服务器通信来获取,但这与K8s的设计理念不符.因此K8s通过一个headless Service创建SRV记录来指向pod的主机名.

可以通过查询DNS记录中的SRV记录,SRV记录用来指向提供服务的服务器的主机和端口号:

```sh
[root@server4-master ~]# kubectl run -it srvlookup --image=tutum/dnsutils --rm --restart=Never -- dig SRV kubia.default.svc.cluster.local

; <<>> DiG 9.9.5-3ubuntu0.2-Ubuntu <<>> SRV kubia.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 27077
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 3
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
;; QUESTION SECTION:
;kubia.default.svc.cluster.local. IN    SRV

;; ANSWER SECTION:
kubia.default.svc.cluster.local. 30 IN  SRV     0 50 80 kubia-1.kubia.default.svc.cluster.local.
kubia.default.svc.cluster.local. 30 IN  SRV     0 50 80 kubia-0.kubia.default.svc.cluster.local.

;; ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. 30 IN A 10.244.244.234
kubia-1.kubia.default.svc.cluster.local. 30 IN A 10.244.244.233

;; Query time: 1 msec
;; SERVER: 10.96.0.10#53(10.96.0.10)
;; WHEN: Fri Mar 18 15:40:53 UTC 2022
;; MSG SIZE  rcvd: 350
```

通过创建一个临时pod来运行dig命令,可以看到ANSWER SECTION显示了两条指向后台headless Service的SRV记录.在ADDITIONAL SECTION中每个pod都拥有独自的一条记录.当一个pod要获取一个StatefulSet里的其他pod列表时,需要做的就是触发一次SRV DNS查询.

### 滚动升级

StatefulSet的升级方式和Deployment一样,推荐先修改配置文件中的镜像或参数,然后使用kubectl apply -f来更新:

```sh
[root@server4-master ~]# vi kubia-st.yaml 
...
  replicas: 3
...
        image: luksa/kubia-pet-peers
...
[root@server4-master ~]# kubectl apply -f kubia-st.yaml 
statefulset.apps/kubia configured
[root@server4-master ~]# kubectl get po
NAME       READY   STATUS        RESTARTS   AGE
dnsutils   1/1     Running       0          43m
kubia-0    1/1     Terminating   0          5h10m
kubia-1    1/1     Running       0          32s
kubia-2    1/1     Running       0          77s
```

### 测试集群数据

当所有pod启动后,可以测试数据储存是否按预期工作.先修改svc为非无头模式后发送一些请求到集群:

```sh
[root@server4-master ~]# vi kubia-svc-peer.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-public
spec:
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
    targetPort: 8080
[root@server4-master ~]# kubectl apply -f kubia-svc-peer.yaml 
service/kubia-public created
[root@k8s-master 2]# curl -X POST -d "11:58:04" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-1
[root@k8s-master 2]# curl -X POST -d "11:58:04" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-1
[root@k8s-master 2]# curl -X POST -d "11:58:04" localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
Data stored on pod kubia-2
```

现在三个Pod中都有数据了,测试从服务中读取数据:

```sh
[root@k8s-master 2]# curl localhost:8001/api/v1/namespaces/default/services/kubia-public/proxy/
You've hit kubia-2
Data stored in the cluster:
- kubia-2.kubia.default.svc.cluster.local: 11:58:04
- kubia-1.kubia.default.svc.cluster.local: 11:58:04
- kubia-0.kubia.default.svc.cluster.local: kubia-0
```

通过集群中任意一个节点都能获取到所有伙伴节点,然后收集它们的数据.

### 处理节点失效

一个有状态pod必须保证创建替代pod之前不再运行.当一个节点突然失效,k8s并不知道节点的状态,不知道pod是否还在运行,是否还存在,是否能被客户端访问,还是Kubelet停止了上报节点状态.只有StatefulSet在明确知道一个pod不再运行后才会有所动作.这个信息由管理员删除pod或整个节点来明确.

模拟一次节点故障.先查看目前运行状态:

```sh
[root@server4-master ~]# kubectl get po -o wide
NAME      READY   STATUS    RESTARTS   AGE   IP               NODE            
kubia-0   1/1     Running   0          29m   10.244.191.226   server5-node1   
kubia-1   1/1     Running   0          30m   10.244.191.225   server5-node1   
kubia-2   1/1     Running   0          12s   10.244.191.227   server5-node1   
```

发现所有pod都运行在node1.把node1的网络断开,稍等2分钟后查询节点信息:

```sh
[root@server4-master ~]# kubectl get node
NAME             STATUS     ROLES                  AGE    VERSION
server4-master   Ready      control-plane,master   135d   v1.22.3
server5-node1    NotReady   <none>                 135d   v1.22.3
server6-node2    Ready      <none>                 135d   v1.22.3
[root@server4-master ~]# kubectl get pod -o wide
NAME      READY   STATUS    RESTARTS   AGE     IP               NODE           
kubia-0   1/1     Running   0          119s    10.244.191.228   server5-node1   
kubia-1   1/1     Running   0          38m     10.244.191.225   server5-node1  
kubia-2   1/1     Running   0          8m10s   10.244.191.227   server5-node1   
```

可以看到node已变为NotReady状态,但pod状态没有更新.如果运行delete命令,因为节点上kubelet无法接收到命令,所以pod的状态会一直显示Terminating.

再过一段时间,pod的状态会变成Unknown,有个配置可以调整未知状态持续多久,pod自动从节点上驱除.

假如节点重连上了,删除命令会被正确执行并在空闲节点上新建pod.如果节点永远消失了,只能通过强制删除pod来解决.通过删除命令加上--force和grace-period 0两个参数来强制删除:

```sh
[root@server4-master ~]# kubectl delete po kubia-0 --force --grace-period 0
warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod "kubia-0" force deleted
[root@server4-master ~]# kubectl get po -o wide
NAME      READY   STATUS    RESTARTS   AGE    IP               NODE            
kubia-0   1/1     Running   0          19s    10.244.244.245   server6-node2   
kubia-1   1/1     Running   0          39m    10.244.191.225   server5-node1   
kubia-2   1/1     Running   0          9m7s   10.244.191.227   server5-node1   
```

pod强制删除后会在node2节点上新建,一般只有确认节点永远不能用的情况下才使用强制删除.

假如把断开网络的node1节点重新连上,pod的状态会从Unknown变为正常运行.
