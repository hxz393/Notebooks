# 储存卷

## emptyDir卷

emptyDir卷是一个空目录,pod内的程序可以写入需要的任何文件,当删除pod时,卷的内容会丢失.主要用在pod中运行的容器之间临时共享文件.

### 使用emptyDir卷

下面使用Nginx作为服务器和Unix fortune命令来生成HTML内容,fortune命令每次运行都会输出一个随机引用,可以创建一个脚本每10秒运行一次,将其储存在index.html:

```sh
[root@server4-master ~]# vi fortune-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    emptyDir: {}                                 
[root@server4-master ~]# kubectl create -f fortune-pod.yaml 
pod/fortune created
```

第一个容器html-generator用来运行生成HTML文件到/var/htdocs中,同时把卷html挂载到目录/var/htdocs.

第二个容器web-server用来运行web服务,同时把卷html挂载到目录/usr/share/nginx/html,权限为只读.

最后定义一个名为html的单独emptyDir卷,供上面容器挂载.

### 查看pod状态

为了测试访问,可以直接设置端口转发来访问pod:

```sh
[root@server4-master ~]# kubectl port-forward fortune 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
```

新开一个终端,在本地通过curl来访问:

```sh
[root@server4-master ~]# curl 127.0.0.1:8080
Few things are harder to put up with than the annoyance of a good example.
                -- "Mark Twain, Pudd'nhead Wilson's Calendar"
```

### 指定储存介质

可以把emptyDir创建在tmfs,也就是内存中,空间受限于内存,但性能非常好,需要同时设置sizeLimit来限制使用.

下面创建一个Pod测试,包含tomcat和busybox两个容器.tomcat向挂载卷写入日志,busybox读取日志:

```sh
[root@server4-master ~]# vi empty-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: volume-pod
  namespace: default
spec:
  containers:
  - name: tomcat
    image: tomcat
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: app-logs
      mountPath: /usr/local/tomcat/logs
  - name: busybox
    image: busybox
    command: ["sh", "-c", "tail -f /logs/catalina*.log"]
    volumeMounts:
    - name: app-logs
      mountPath: /logs
  volumes:
  - name: app-logs
    emptyDir:
      medium: Memory
[root@server4-master ~]# kubectl create -f empty-pod.yaml 
pod/volume-pod created
```

查看日志:

```sh
[root@server4-master ~]# kubectl logs volume-pod -c busybox
14-Mar-2022 03:49:09.120 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Command line argument: -Djava.io.tmpdir=/usr/local/tomcat/temp
14-Mar-2022 03:49:09.128 INFO [main] org.apache.catalina.core.AprLifecycleListener.lifecycleEvent Loaded Apache Tomcat Native library [1.2.31] using APR version [1.7.0].
```



## gitRepo卷

gitRepo卷基本上也是一个emptyDIr卷,通过pod启动时克隆Git仓库来填充数据.gitRepo卷创建完毕后,仓库并不会随着Git远程仓库更新,如果使用RS管理,删除pod将会新建pod,再次触发git clone可以获得最新文件.

一个典型的应用场景是存放网站的静态HTML文件,并创建一个包含web服务器容器和gitRepo卷的pod,当pod创建时,会拉取网站最新版本并启动web服务.不过每次有新版本需要删除pod才会更新.

### 使用gitRepo卷

使用一个Nginx容器和gitRepo卷:

```sh
[root@k8s-master ~]vi gitrepo-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: gitrepo-volume-pod
spec:
  containers:
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: true
    ports:
    - containerPort: 80
      protocol: TCP
  volumes:
  - name: html
    gitRepo:
      repository: https://github.com/luksa/kubia-wesite-example.git
      revision: master
      directory: .
[root@k8s-master ~]# kubectl create -f gitrepo-pod.yaml 
pod/gitrepo-volume-pod created
```

在gitRepo中需要revision指定master分支,同时将repo克隆到根目录.也可以指定文件夹名称.

### 私有仓库

gitRepo卷不能拉取私有Git repo仓库,例如需要账号密码验证的gitlab.如果需要添加支持,需要在pod中添加额外的sidecar容器,例如gitsync sidecar能对pod主容器操作同步仓库版本等.

gitRepo卷现在已被废弃,官方建议使用初始化容器将仓库中的数据复制到emptyDir储存卷上.



## 底层持久化储存

### hostPath卷

hostPath卷指向节点文件系统上特定文件或目录,文件被储存在特定节点的文件系统中,所以当pod被重新安排在另一个节点时会找不到数据.
在k8s系统级服务pod中广泛使用hostPath卷来访问节点的日志,配置文件或CA证书,但它们并不使用hostPath卷来储存数据:

```sh
volumes:
- name: "hostpath"
  hostPath:
    path: "/data"
```

还可用type参数指定储存卷类型:

- DirectoryOrCreate

  指定路径不存在时自动创建为权限0755的空目录,属组和属主为kubelet.

- DIrectory

  必须存在的目录路径.

- FileOrCreate

  指定的路径不存在时自动创建为权限0644的空文件,属组和属主为kubelet.

- File

  必须存在的文件路径.

- Socket

  必须存在的Socket文件路径.

- CharDevice

  必须存在的字符设备文件路径.

- BlockDevice

  必须存在的块设备文件路径.

### GCE持久储存

如果集群运行在Google Kubernetes Engine中,那么可以选择GCE持久磁盘作为底层储存机制.
首先需要在同一区域的k8s集群中创建它,例如europe-west区,然后创建一个带GCE持久磁盘卷的pod:

```sh
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data.db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
      pdName: mongodb
      fsType: ext4
```

定义名称和文件系统类型为EXT4,接着向mongodb写入数据:

```sh
[root@k8s-master ~]kubectl exec -it mongodb mongo
[root@mongodb ~]use mystore
[root@mongodb ~]db.foo.insert({name:'foo'})
[root@mongodb ~]db.foo.find()
```

重建pod后读取上一个pod保存的数据:

```sh
[root@k8s-master ~]kubectl delete pod mongodb
[root@k8s-master ~]kubectl create -f mongodb.yaml
[root@k8s-master ~]kubectl exec -it mongodb mongo
[root@mongodb ~]use mystore
[root@mongodb ~]db.foo.find()
```

如果数据仍在说明持久化成功.

### 其他持久化储存卷

根据不同的基础设施使用不同类型卷,比如在Amazon上可以使用awsElasticBlockStore卷,在Microsoft Azure上使用azureFile或azureDisk卷,例如aws中:

```sh
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data
    awsElasticBlockStore:
      volumeID: mongodb
      fsType: ext4
...
```

在NFS共享中,只需要指定NFS服务器IP地址和共享路径:

```sh
...
spec:
  volumes:
  - name: mongodb-data
    nfs:
      server: 1.2.3.4
      path: /some/path
...
```

要了解每个卷类型设置需要哪些属性可以查询Kubernetes API,或使用kubectl explain命令.

将这些基础设施类型放到一个pod设置中意味着pod设置于特定的集群有很大耦合度,这样不能在另一个pod中使用相同的设置了.



## 持久卷和持久卷声明

在集群中为了使应用正常请求储存资源,同时避免处理基础设施细节,引入了两个新资源,分别是持久卷(PersistentVolume, PV)和持久卷声明(PersistentVolumeClaim, PVC).

当集群用户需要在其pod中使用持久化储存时,首先创建持久卷声明清单,指定所需最低容量要求和访问模式,由API服务器分配持久卷并绑定到持久卷声明中.

持久卷声明可以当做pod中的一个卷来使用,其他用户不能使用相同的持久卷,除非先通过删除持久卷声明释放.

### 安装NFS文件系统

在所有节点上安装:

```sh
[root@server4-master ~]# yum install nfs-utils rpcbind -y
[root@server4-master ~]# systemctl start nfs && systemctl enable nfs
Created symlink from /etc/systemd/system/multi-user.target.wants/nfs-server.service to /usr/lib/systemd/system/nfs-server.service.
```

在nfs服务器上启动:

```sh
[root@server4-master ~]# systemctl restart rpcbind && systemctl enable rpcbind
[root@server4-master ~]# systemctl restart nfs && systemctl enable nfs
```

配置服务器上的共享目录:

```sh
[root@server4-master ~]# mkdir /srv/pv
[root@server4-master ~]# chown nfsnobody:nfsnobody /srv/pv
[root@server4-master ~]# chmod 755 /srv/pv
[root@server4-master ~]# echo -e "/srv/pv *(rw,no_root_squash,sync)">/etc/exports
[root@server4-master ~]# exportfs -r
[root@server4-master ~]# exportfs
/srv/pv         <world>
[root@server4-master ~]# showmount -e 192.168.2.204
Export list for 192.168.2.204:
/srv/pv *
```

修改最大同时连接用户数:

```sh
[root@server4-master ~]# echo "options sunrpc tcp_slot_table_entries=128" >> /etc/modprobe.d/sunrpc.conf
[root@server4-master ~]# echo "options sunrpc tcp_max_slot_table_entries=128" >>  /etc/modprobe.d/sunrpc.conf
[root@server4-master ~]# sysctl -w sunrpc.tcp_slot_table_entries=128
sunrpc.tcp_slot_table_entries = 128
```

创建新的挂载点:

```sh
[root@server4-master ~]# mkdir -p /srv/pv/pv001 /srv/pv/pv002
[root@server4-master ~]# echo -e "/srv/pv/pv001 *(rw,no_root_squash,sync)">>/etc/exports
[root@server4-master ~]# echo -e "/srv/pv/pv002 *(rw,no_root_squash,sync)">>/etc/exports
[root@server4-master ~]# exportfs -r
[root@server4-master ~]# systemctl restart rpcbind && systemctl restart nfs
[root@server4-master ~]# showmount -e 192.168.2.204
Export list for 192.168.2.204:
/srv/pv/pv002 *
/srv/pv/pv001 *
/srv/pv       *
```

### 创建持久卷

配置一个1GB大小供mongodb使用的持久卷:

```sh
[root@server4-master ~]# vi mongodb-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mongodb-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  - ReadOnlyMany
  persistentVolumeReclaimPolicy: Retain
  nfs:
    path: /srv/pv
    server: 192.168.2.204
[root@server4-master ~]# kubectl create -f mongodb-pv.yaml 
persistentvolume/mongodb-pv created
[root@server4-master ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
mongodb-pv   1Gi        RWO,ROX        Retain           Available                                   4s
```

持久卷不属于任何命名空间,它和节点一样是集群层面的资源.

### 创建持久卷声明

如果pod需要用到之前创建的持久卷,需要建立一个持久卷声明:

```sh
[root@server4-master ~]# vi mongodb-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  resources:
    requests:
      storage: 1Gi
  accessModes:
  - ReadWriteOnce
  storageClassName: ""
[root@server4-master ~]# kubectl create -f mongodb-pvc.yaml
persistentvolumeclaim/mongodb-pvc created
[root@server4-master ~]# kubectl get pvc
NAME          STATUS   VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS   AGE
mongodb-pvc   Bound    mongodb-pv   1Gi        RWO,ROX                       4s

```

查看PVC状态可以看到PVC已经与对应的PV绑定.其中访问模式简写含义如下:

- RWO(ReadWriteOnce):仅允许单个节点挂载读写.

- ROX(ReadOnlyMany):允许多个节点挂载只读.

- RWX(ReadWriteMany):允许多个节点挂载读写.

其中节点代表着k8s节点,而不是pod的数量.

```sh
[root@server4-master ~]# kubectl get pv
NAME         CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
mongodb-pv   1Gi        RWO,ROX        Retain           Bound    default/mongodb-pvc                           12m
```

再次查看pv状态可以看到已经被绑定到pvc的声明中,其中CLAIM中的default表示默认命名空间.虽然持久卷属于集群,但持久卷声明只能在特定的命名空间创建,所以持久卷和持久卷声明只能被同一命名空间内的pod创建使用.还可以使用selector来对PV应用标签选择器.

### 使用持久卷声明

在pod中使用持久卷,需要在pod的卷中引用PVC名称:

```sh
[root@server4-master ~]# vi mongodb-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP
  volumes:
  - name: mongodb-data
    persistentVolumeClaim:
      claimName: mongodb-pvc                                    
[root@server4-master ~]# kubectl create -f mongodb-pod.yaml 
pod/mongodb created
```

注意,虽然PV是全局资源,但PVC是属于命名空间,只有同一命名空间的Pod才可调用.

### 删除持久卷

在持久卷被使用时不能直接删除,需要先删除使用的pod和pvc.空间释放处理机制三种:Retain, Recycle, Delete.

删除pv后,依据设置的释放规则Retain,硬盘中的文件依然存在,重新创建pv,pvc,pod后文件和上一次运行一样.使用Retain手动回收策略只有删除和重建持久卷才能恢复可用.

另外还有Recycle策略删除PVC后会删除卷的内容并使卷可用于再次声明.和Delete策略删除底层储存.



## 动态化持久卷

在K8s中可以通过创建一个持久卷配置,并定义一个或多个StorageClass对象,从而让用户每次通过持久卷声明请求时自动创建一个新的持久卷.

不同的后端储存需要不同的置备程序(provisioner).下面以NFS文件系统为例,首先部署nfs-client:

```sh
[root@k8s-master html]# echo "/root/3/html *(rw,sync,no_root_squash)" >> /etc/exports
[root@k8s-master ~]# vi nfs-dp.yaml
kind: Deployment
apiVersion: extensions/v1beta1
metadata:
  name: nfs-client-provisioner
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccount: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: jmgao1983/nfs-client-provisioner
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: mynfs
            - name: NFS_SERVER
              value: 192.168.2.113
            - name: NFS_PATH
              value: /srv/pv
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.2.113
            path: /srv/pv
[root@k8s-master ~]# kubectl create -f nfs-dp.yaml
deployment.extensions/nfs-provisioner created
[root@k8s-master ~]# kubectl get deployment
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
nfs-provisioner   1/1     1            1           29s
```

建立StorageClass资源:

```sh
[root@k8s-master ~]# vi mongodb-pv-sc.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: mynfs
[root@k8s-master ~]# kubectl create -f mongodb-pv-sc.yaml
storageclass.storage.k8s.io/fast created
[root@k8s-master ~]# kubectl get sc
NAME   PROVISIONER   AGE
fast   mynfs         9s
```

创建一个pod引用SC:

```sh
[root@k8s-master ~]# vi nginx.yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx1"
  replicas: 2
  volumeClaimTemplates:
  - metadata:
      name: test
      annotations:
        volume.beta.kubernetes.io/storage-class: "fast"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi
  template:
    metadata:
      labels:
        app: nginx1
    spec:
      containers:
      - name: nginx1
        image: nginx:latest
        volumeMounts:
        - mountPath: "/mnt"
          name: test
"nginx.yaml" [New] 28L, 556C written
[root@k8s-master ~]# kubectl create -f nginx.yaml
statefulset.apps/web created
```

创建ServiceAccount和角色:

```sh
[root@k8s-master ~]# vi serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
[root@k8s-master ~]# kubectl create -f serviceaccount.yaml
serviceaccount/nfs-client-provisioner created
[root@k8s-master ~]# kubectl get sa
NAME                     SECRETS   AGE
default                  1         3d14h
nfs-client-provisioner   1         16s
[root@k8s-master ~]# vi clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["watch", "create", "update", "patch"]
  - apiGroups: [""]
    resources: ["services", "endpoints"]
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources: ["podsecuritypolicies"]
    resourceNames: ["nfs-provisioner"]
    verbs: ["use"]
"clusterrole.yaml" [New] 24L, 735C written
[root@k8s-master ~]# vi clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
[root@k8s-master ~]# kubectl create -f clusterrole.yaml -f clusterrolebinding.yaml
clusterrole.rbac.authorization.k8s.io/nfs-provisioner-runner created
clusterrolebinding.rbac.authorization.k8s.io/run-nfs-provisioner created
```

验证一下是不是会自动新建PV:

```sh
[root@k8s-master ~]# kubectl get pv |grep web
pvc-d2423554-6b6e-4c65-9209-e739abe7c653   1Gi        RWO            Delete           Bound       default/test-web-0   fast                    2m28s
pvc-d430fa25-d4e4-4971-961f-80be40b8d9dc   1Gi        RWO            Delete           Bound       default/test-web-1   fast                    2m12s
[root@k8s-master ~]# kubectl get pvc |grep web
test-web-0   Bound    pvc-d2423554-6b6e-4c65-9209-e739abe7c653   1Gi        RWO            fast           13m
test-web-1   Bound    pvc-d430fa25-d4e4-4971-961f-80be40b8d9dc   1Gi        RWO            fast           2m36s
[root@k8s-master ~]# kubectl get storageclass
NAME   PROVISIONER   AGE
fast   mynfs         19m
[root@k8s-master ~]# kubectl get pod |grep web
web-0                                    1/1     Running   0          8m43s
web-1                                    1/1     Running   0          3m29s
扩展pod
[root@k8s-master ~]# kubectl scale statefulset web --replicas=3
statefulset.apps/web scaled
[root@k8s-master ~]# kubectl get pod |grep web
web-0                                    1/1     Running             0          9m32s
web-1                                    1/1     Running             0          4m18s
web-2                                    0/1     ContainerCreating   0          4s
[root@k8s-master ~]# ll /srv/pv/
total 0
drwxrwxrwx 2 root root 6 Jul 22 17:34 default-test-web-0-pvc-d2423554-6b6e-4c65-9209-e739abe7c653
drwxrwxrwx 2 root root 6 Jul 22 17:35 default-test-web-1-pvc-d430fa25-d4e4-4971-961f-80be40b8d9dc
drwxrwxrwx 2 root root 6 Jul 22 17:39 default-test-web-2-pvc-ffc2b763-2dd8-4a5b-a565-86dd416eba36
```



## ConfigMap资源

应用配置的关键在于能够在多个环境中区分配置选项,将配置从应用程序源码中分离,K8s允许将配置选项分离到单独的资源对象ConfigMap中.ConfigMap本质上就是一个键值对映射,值可以是字符串,也可以是完整的配置文件.

应用无须直接读取ConfigMap,映射的内容通过环境变量或卷文件的形式传给容器,命令行参数的定义中可以通过引用变量语法引用环境变量,因而可以达到将ConfigMap的条目当做命令行参数传递给进程的效果.

pod通过名称引用ConfigMap,因此可以在多环境中使用相同的pod定义描述,同时配置值不同以适应不同环境.

### 指定容器环境变量

如果Docker镜像有自定义参数可以配置,如下Dockerfile中所示:

```sh
#!/bin/bash
trap "exit" SIGINT
echo Fortune sleep every $INTERVAL seconds
mkdir -p /bar/htdocs
while :
do
  echo $(date) Writing fortune to /var/htdocs/index.html
  /usr/games/fortune > /var/htdocs/index.html
  sleep $INTERVAL
done
```

那么想要在k8s中设置自定义参数可以在spec.containers.env中声明:

```sh
spec:
  containers:
  - image: fortune
    env:
    - name: INTERVAL
      value: "30"
    name: html-generator
```

也可以使用$(VAR)语法在环境变量值中引用其他环境变量:

```sh
spec:
  containers:
  - image: fortune
    env:
    - name: FIRST_VAR
      value: "foobar"
    - name: SECOND_VAR
      value: "$(FIRST_VAR)2000"
```

### 创建ConfigMap

可以直接通过命令行来创建一个最简单的ConfigMap:

```sh
[root@server4-master ~]# kubectl create configmap fortune-config --from-literal=sleep-interval=5
configmap/fortune-config created
```

这条指令创建了名为fortune-config的configmap,仅包含单映射条目sleep-interval=5.可以通过添加多个--from-literal参数创建包含多条目的ConfigMap:

```sh
[root@server4-master ~]# kubectl create configmap fortune-config --from-literal=one=1 --from-literal=two=11
```

通过观察YAML格式的定义描述,可以自定义配置文件,通过create -f来创建ConfigMap.

可以直接从硬盘中读取文件,并将文件内容单独储存为ConfigMap中的值.如把当前目录下my.config文件内容存为键名devconfig的值.:

```sh
[root@k8s-master 2]# kubectl create configmap my-config --from-file=devconfig=my.config
```

通过多次使用--from-file参数可以增加多个文件条目.另外还可以用--from-file指定目录,kubectl会为文件夹中的每个文件单独创建条目,键名为文件名:

```sh
[root@server4-master ~]# kubectl create configmap my-config-dir --from-file=k8s
configmap/my-config-dir created
```

ConfigMap可以混合使用多种类型配置,例如一个my-config同时包含键值对和文件:

```sh
[root@server4-master ~]# kubectl create configmap my-config --from-file=foo.json --from-file=bar=foobar.conf --from-file=config-opts/ --from-literal=some=thing
```

### 传递ConfigMap

将值传给pod中的容器有三种方式.如果引用的ConfigMap不存在,容器会启动失败,也可以设置optional: true对引用可选.

- 通过环境变量传递键值

  需要在spec.containers.env.valueFrom字段中指定:
  
  ```sh
  spec:
    containers:
    - image: fortune
      env:
      - name: INTERVAL
        valueFrom:
          configMapKeyRef:
            name: fortune-config
            key: sleep-interval
  ```
  
  这里传递了一个环境变量INTERVAL,值取自fortune-config中键sleep-interval的值,再由容器内的进程读取.
  

- 传递整个ConfigMap中的键值

  在spec.containers中加入envFrom字段来传递整个ConfigMap中的键值对:

  ```sh
  spec:
    containers:
    - image: fortune
      envFrom:
      - prefix: CONFIG_
        configMapRef:
          name: my-cofig-map
  ```

  上面设置所有导入的环境变量包含前缀CONFIG_,若不设置前缀,环境变量的名称与ConfigMap中的键名相同,若键名不合法时不会自动转换.

- 传递ConfigMap条目作为命令行参数

  在字段spec.containers.args中无法直接引用ConfigMap的条目,但可以利用ConfigMap条目初始化某个环境变量,然后再在参数字段中引用该变量:

  ```
  spec:
    containers:
    - image: fortune
      env:
      - name: INTERVAL
        valueFrom:
          configMapKeyRef:
            name: fortune-config
            key: sleep-interval
      args: ["$(INTERVAL)"]
  ```

### configMap卷

由于ConfigMap中可以包含完整配置文件内容,想要暴露给容器时可以借助configMap卷.configMap卷会将ConfigMap中的每个条目暴露出一个文件,运行在容器中的进程通过读取文件内容获得对应条目值.

例如将目录中的nginx配置文件和interval文件一起创建名为fortune-config的ConfigMap:

```sh
[root@server4-master configmap-files]# echo "25" > interval
[root@server4-master configmap-files]# vi advertise-task.iot.com.conf 
server {
    listen 80;
    server_name advertise-task.iot.com;
    location / {
        proxy_pass http://advertise-task.iot.com.dev;
    }
}
[root@server4-master configmap-files]# kubectl create configmap fortune-config --from-file=../configmap-files/
configmap/fortune-config created
[root@server4-master ~]# kubectl get configmaps fortune-config -o yaml
apiVersion: v1
data:
  interval: |
    25
  advertise-task.iot.com.conf: |
    server {
        listen 80;
        server_name advertise-task.iot.com;
        location / {
            proxy_pass http://advertise-task.iot.com.dev;
        }
    }
kind: ConfigMap
metadata:
  creationTimestamp: "2022-03-15T14:01:15Z"
  name: fortune-config
  namespace: default
  resourceVersion: "580367"
  uid: 9a4c9cb4-c2d2-424c-9d57-ac3fe1851260
```

将configMap卷内文件挂载到/etc/nginx/conf.d/目录下:

```sh
spec:
  containers:
  - image: nginx:alpine
    name: web-server
	volumeMounts:
	- name: config
	  mountPath: /etc/nginx/conf.d
	  readOnly: true
  volumes:
    - name: config
	  configMap:
	    name: fortune-config
```

也可以单独指定需要挂载的configMap卷内文件.这里将configMap卷内配置文件advertise-task.iot.com.conf重命名为at.conf并挂载到指定目录下.:

```sh
spec:
  volumes:
    - name: config
	  configMap:
	    name: fortune-config
	    items:
	    - key: advertise-task.iot.com.conf
	      path: at.conf
```

使用上面方法挂载卷,容器内本身包含同名目录会隐藏,被挂载的目录替代.想要单独挂载文件而不是目录,需要在spec.containers.volumeMounts下面加入subPath字段:

```sh
spec:
  containers:
  - image: nginx:alpine
    name: web-server
	volumeMounts:
	- name: config
	  mountPath: /etc/nginx/conf.d/advertise-task.iot.com.conf
	  subPath: advertise-task.iot.com.conf
	  readOnly: true
```

configMap卷中所有文件的默认权限是644,可以在spec.volumes.configMap中加入defaultMode来改变默认权限:

```sh
spec:
  volumes:
    - name: config
	  configMap:
	    name: fortune-config
	    defaultMod: "6600"
```

将ConfigMap作为卷挂载可以达到配置热更新的效果,无需重启或重建pod.在修改了ConfigMap配置后,可以通过kubectl exec命令来手动载入更新的配置:

```sh
[root@server4-master ~]# kubectl edit configmap fortune-config
[root@server4-master ~]# kubectl exec fortune -c web-server -- nginx -s reload
```

但如果挂载的单个文件,ConfigMap更新后对应的文件不会被封信.



## Secret资源

Secret的结构和使用方法与ConfigMap相同,也是键值对的映射,主要用来保存敏感信息,比如账号和证书.

k8s通过将Secret分发到pod所需节点来保障安全性,Secret只会储存在节点的内存中,这样就无法被窃取.

### 默认令牌

K8s安装完毕后会生产一个以default-token开头,默认挂载至所有容器的Secret:

```sh
[root@server4-master ~]# kubectl get secrets 
NAME                  TYPE                                  DATA   AGE
default-token-vqjsw   kubernetes.io/service-account-token   3      132d
[root@server4-master ~]# kubectl describe secrets default-token-vqjsw
Name:         default-token-vqjsw
Namespace:    default
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: default
              kubernetes.io/service-account.uid: ae0b90b8-1323-42a2-990b-a18d4bfb7da8

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1099 bytes
namespace:  7 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlZDd
```

默认Secret包含三个条目: ca.crt, namespace和token,包含从pod内部安全访问API服务器所需的全部信息.

可以在po信息的mount栏看到secret卷被挂载的位置:

```sh
[root@server4-master ~]# kubectl describe po dnsutils 
Containers:
  dnsutils:
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5ghsj (ro)
```

默认挂载行为可以通过设置pod定义中的automountServiceAccountToken字段为false来关闭.

### 创建Secret

用命令行来建立一个名为ngx-https的Secret来保存Nginx所需的私钥和证书:

```sh
[root@server4-master ~]# kubectl create secret generic ngx-https --from-file=https.cert --from-file=https.key --from-file=foo
```

Secret条目的内容都会被以Base64格式编码,而ConfigMap直接以纯文本展示.所以Secret甚至可以用来储存最大1MB的二进制数据.

通过stringData字段设置条目的纯文本值,不会被base64编码:

```sh
[root@server4-master ~]# vi secret-test.yaml
apiVersion: v1
kind: Secret
stringData:
  foo: ymfyc
data:
  https.cert: Ls-3lcc
  https.key: Lsv9elx
[root@server4-master ~]# kubectl create -f secret-test.yaml 
```

注意stringData字段是只写的.

### 使用Secret

通过secret卷将Secret暴露给容器之后,Secret条目的值会被解码为真实形式(纯文本或二进制)写入对应的文件.通过环境变量暴露Secret条目亦是如此.在这两种情况下,应用程序均无须主动解码,可直接读取文件内容或查找环境变量:

```sh
spec:
  containers:
	volumeMounts:
	- name: certs
	  mountPath: /etc/nginx/certs/
	  readOnly: true
  volumes:
    - name: certs
	  secret:
	    secretame: ngx-https
```

与configMap卷相同,secret卷同样支持通过defaultModes属性指定卷中文件的默认权限.

Secret条目也可以暴露为环境变量:

```sh
spec:
  containers:
    env:
    - name: INTERVAL
      valueFrom:
	    secretKeyRef:
		  name: ngx-https
		  key: foo
```

可以创建一个包含Docker镜像仓库证书的Secret,类型为docker-registry,名叫dockerhubsecret.并在拉取镜像时引用:

```sh
[root@k8s-master 2]# kubectl create secret docker-registry dockerhubsecret \
 --docker-username=myusername --docker-password=mypasswd \
 --docker-email=my@email.com
[root@k8s-master 2]# vi dockerhub.yaml
apiVersion: v1
kind: Pod
metadata:
  name: privae-pod
spec:
  imagePullSecrets:
  - name: dockerhubsecret
  containers:
  - image: assassing/av:v1
    name: main
```

