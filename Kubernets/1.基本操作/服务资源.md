# 服务资源

## 基本操作

服务services使用标签来选择pod.下面简称SVC.

### 创建SVC

假设已经启动了一组pod,标签为app=kubia,可以通过命令kubectl expose来创建服务.但更推荐使用YAML文件:

```sh
[root@server4-master ~]# vi kubia-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
```

其中80端口用来给外界访问,8080映射容器端口,并通过标签app=kubia选择pod.

创建并查看服务:

```sh
[root@server4-master ~]# kubectl create -f kubia-svc.yaml 
[root@server4-master ~]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   17h
kubia        ClusterIP   10.107.244.106   <none>        80/TCP    8s
```

可以看到服务已经分配了一个内部集群的IP地址10.107.244.106,访问端口为80.

### 测试服务

由于是内部集群地址,只有集群中的pod可以访问.可以使用kubectl exec命令在pod中执行curl命令:

```sh
[root@server4-master ~]# kubectl exec kubiaex-f4wkw -- curl -s 10.107.244.106
You've hit kubiaex-nj968
[root@server4-master ~]# kubectl exec kubiaex-f4wkw -- curl -s 10.107.244.106
You've hit kubiaex-ptws9
```

从结果可以看到访问连到后端随机一个pod中.

### 亲和性

可以配置服务的会话亲和性,这样会使服务代理将同IP的请求转发到同一个pod中:

```sh
[root@server4-master ~]# vi kubia-svc.yaml
spec:
  sessionAffinity: ClientIP
```

亲和性配置sessionAffinity值默认为None.由于K8s服务不是在HTTP层面上工作,它只处理TCP和UDP包,所以会话亲和性不能基于cookie.

### 暴露多端口

可以在服务中暴露多个端口.必须设置端口名称:

```sh
[root@server4-master ~]# vi kubia-svc.yaml
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
```

### 命名引用

如果pod配置中对端口取了名:

```sh
[root@server4-master ~]# vi kubia-rs.yaml
spec:
  containers:
    ports: 
    - name: http
      containerPort: 8080
    - name: https
      containerPort: 8443
```

那么在服务配置中可以直接引用:

```sh
[root@server4-master ~]# vi kubia-svc.yaml
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  - name: https
    port: 443
    targetPort: https
```

这样即使容器更换端口号也无须更改服务的配置.



## 服务发现

服务可以提供稳定的IP地址来访问pod,只要服务存在IP就不会变.但客户端pod要获得服务的IP和端口需要配置.

### 环境变量

如果服务先于pod创建,pod创建时K8s会初始化一系列环境变量指向现存的服务,pod上的进程可以根据环境变量获得服务的IP地址和端口号.

重新运行pod后再查看环境变量:

```sh
[root@server4-master ~]# kubectl delete -f kubia-rs.yaml 
[root@server4-master ~]# kubectl create -f kubia-rs.yaml 
[root@server4-master ~]# kubectl exec kubiaex-5s68q -- env
KUBIA_PORT=tcp://10.107.244.106:80
KUBIA_SERVICE_HOST=10.107.244.106
KUBIA_SERVICE_PORT=80
KUBIA_PORT_80_TCP_PROTO=tcp
KUBIA_PORT_80_TCP_ADDR=10.107.244.106
KUBIA_PORT_80_TCP=tcp://10.107.244.106:80
KUBIA_PORT_80_TCP_PORT=80
```

上面显示的就是创建服务之后才有的环境变量,指向了服务kubia的IP地址和端口.此外还有kubernetes服务的环境变量.

### DNS

K8s使用coredns作为默认的DNS服务,每个运行的pod都会在/etc/resolv.conf中写入DNS服务的地址:

```sh
[root@server4-master ~]# kubectl -n kube-system describe service/kube-dns
IP:                10.96.0.10
[root@server4-master ~]#  kubectl exec -it kubiaex-5s68q -- cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

pod是否使用内部DNS策略由spec.dnsPolicy来决定:

```sh
[root@server4-master ~]# kubectl get po kubiaex-5s68q -o yaml
spec:
  dnsPolicy: ClusterFirst
```

客户端的pod在知道服务名称的情况下可以通过全域名限定FQDN来访问,而不用依靠环境变量.例如svc的全名是kubia.default.svc.cluster.local,通过pod中容器去访问这个地址:

```sh
root@kubiaex-5s68q:/# curl http://kubia.default.svc.cluster.local
You've hit kubiaex-bwzb9
```

FQDN分为三部分:kubia为主机名, default为对应命名空间, svc.cluster.local是所有集群本地服务名称中使用的可配置集群域后缀.

如果要通信的资源在一个命名空间下,可以省略后缀,直接通过主机名通信:

```sh
root@kubiaex-5s68q:/# curl http://kubia  
You've hit kubiaex-bwzb9
root@kubiaex-5s68q:/# ping kubia
PING kubia.default.svc.cluster.local (10.107.244.106): 56 data bytes
```



## 指向外部服务

SVC也可以将服务的地址重定向到集群外部网络的地址和端口,在集群中的pod可以像连接内部服务一样连接到外部服务.

### Endpoint

SVC通过Endpoint(下面简称EP)资源来和pod连接,一般指向pod的IP地址和端口列表:

```sh
[root@server4-master ~]# kubectl describe svc kubia
Endpoints:         10.244.191.210:8080,10.244.191.211:8080,10.244.244.209:8080
```

也可以直接查询EP资源:

```sh
[root@server4-master ~]# kubectl get endpoints kubia
NAME    ENDPOINTS                                                     AGE
kubia   10.244.191.210:8080,10.244.191.211:8080,10.244.244.209:8080   100m
[root@server4-master ~]# kubectl describe endpoints kubia
Name:         kubia
Namespace:    default
Labels:       <none>
Annotations:  endpoints.kubernetes.io/last-change-trigger-time: 2021-11-03T11:45:07Z
Subsets:
  Addresses:          10.244.191.210,10.244.191.211,10.244.244.209
  NotReadyAddresses:  <none>
  Ports:
    Name     Port  Protocol
    ----     ----  --------
    <unset>  8080  TCP
```

EP的作用就是储存通过pod选择器得到的pod地址和端口,提供给服务调用.

### 配置EP

可以分别创建SVC和EP资源来自定义要转发到地址列表.先创建一个没有选择器的服务,并指明EP目标端口为80:

```sh
[root@server4-master ~]# vi outside-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: outside
spec:
  ports:
  - name: http
    port: 80
```

创建一个EP来指定地址列表 .名称需要和SVC的一致:

```sh
[root@server4-master ~]# vi outside-ep.yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: outside
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80
```

分别部署后,SVC就具有了转发到外部服务的功能:

```sh
[root@server4-master ~]# kubectl create -f outside-svc.yaml -f outside-ep.yaml 
```

自定义Endpoint通常可用于服务迁移.当集群内的应用迁移到外部时,修改EP资源中的地址后,SVC提供的访问IP地址可以保持不变.

### 配置别名

除了手动配置EP外,还可以设置域名作为转发目标.只需要在SVC配置文件中设置类型为ExternalName即可:

```sh
[root@server4-master ~]# vi ext-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: exten
spec:
  type: ExternalName
  externalName: someapi.somecompany.com
  ports:
  - port: 80
    targetPort: 6379
[root@server4-master ~]# kubectl create -f ext-svc.yaml 
[root@server4-master ~]# kubectl get svc exten 
NAME    TYPE           CLUSTER-IP   EXTERNAL-IP               PORT(S)   AGE
exten   ExternalName   <none>       someapi.somecompany.com   80/TCP    29s
```

之后集群内的pod可以通过访问exten这个SVC就能连接到外部域名someapi.somecompany.com.

这样的好处是不需要关心外部域名的IP地址,即使IP有变动也不会影响SVC的运行.



## 暴露内部服务

暴露端口使用最多的是NodePort,一般应用端口可以通过Nginx做反向代理而不需要暴露出去.

### NodePort

NodePort类型服务除了通过任何节点的IP和端口访问外,还可以通过服务的内部集群IP来访问,两者不互相冲突.默认情况下NodePort可用端口范围是30000到32767.

创建服务配置文件,除了设置类型为NodePort外,还需要定义到3个端口:

```sh
[root@server4-master ~]# vi kubia-np.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia
```

其中port端口80代表集群内部访问端口,targetPort端口8080代表pod中服务监听的端口,这两个端口和普通服务定义一样.多出来的nodePort端口30123是给外部访问时开放的端口.如果忽略设置nodePort,K8s会使用随机端口.

```sh
[root@server4-master ~]# kubectl create -f kubia-np.yaml
service/kubia-nodeport created
```

启动后,通过浏览器访问地址http://任意节点IP:30123/便能打开服务.可以通过JSONPath选择器,来获取所有节点IP:

```sh
[root@server4-master ~]# kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
192.168.2.204 192.168.2.205 192.168.2.206
```

此种方式当有节点发生故障,节点将不能访问,应当优先使用LoadBalance.

### LoadBalance

负载均衡器拥有独一无二的IP地址,并将所有连接重定向到服务.LoadBalancer服务是NodePort服务的扩展,如果k8s在不支持Load Balancer服务的环境中运行,服务仍会运作在NodePort模式.

```sh
[root@localhost ~]# vi kubia-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
[root@localhost ~]# kubectl create -f kubia-lb.yaml 
service/kubia-loadbalancer created
```

所需要的只是设置好服务类型为LoadBalancer,服务会从集群基础构架获取到负载平衡器,并将其IP地址写入到服务EXTERNAL-IP.

通过浏览器访问会发现每次浏览器都会碰到同一个pod,因为服务在连接级别工作,所以首次打开连接时会选择一个随机集群,之后会将所有数据包全部发送到单个集群直到连接关闭.

### Ingress

使用Ingress只需要一个公网IP就能为所有服务提供访问,Ingress会根据请求的主机名和路径决定转发到的服务.

Ingress作用在网络第七层,所以能实现基于cookie的会话亲和性等功能,转发效率上低于作用于第四层协议的服务.

Ingress资源必须配合Ingress控制器才能正常工作,通过通过官网提供yaml直接部署Ingress,可以基于Nginx, Envoy, Haproxy, Vulcand和Traefik等作为反向代理:

```sh
[root@localhost ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
namespace/ingress-nginx created
configmap/nginx-configuration created
configmap/tcp-services created
configmap/udp-services created
serviceaccount/nginx-ingress-serviceaccount created
clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
role.rbac.authorization.k8s.io/nginx-ingress-role created
rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
deployment.apps/nginx-ingress-controller created
```

查看yaml内容,会自动创建namespace,configmap等等服务和控制器.查看运行状态:

```sh
[root@localhost ~]# kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx --watch
NAMESPACE       NAME                                        READY   STATUS              RESTARTS   AGE
ingress-nginx   nginx-ingress-controller-7995bd9c47-8v8rj   0/1     ContainerCreating   0          75s
ingress-nginx   nginx-ingress-controller-7995bd9c47-8v8rj   0/1     Running             0          88s
```

一个属于ingress-nginx命名空间的nginx-ingress-controller控制器pod便部署完成.接着创建服务:

```sh
[root@localhost ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml
service/ingress-nginx created
[root@k8s-master ~]# kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-ingress-controller-7995bd9c47-8v8rj   1/1     Running   0          3h6m
[root@k8s-master ~]# kubectl get svc -n ingress-nginx
NAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx   NodePort   10.98.41.161   <none>        80:30112/TCP,443:30970/TCP   105m
```

建立yaml文件,绑定后端服务:

```sh
[root@localhost ~]# vi kubia-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia-ingress
spec:
  rules:
  - host: kubia.exp.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia
          servicePort: 80
```

把虚假域名kubia.exp.com映射到Ingress控制器的节点IP(192.168.2.113-115 kubia.exp.com).配置将所有请求发送到kubia服务的80端口.云服务商要求Ingress指向一个NodePort服务,但实际可以指向任意服务:

```sh
[root@localhost ~]# kubectl create -f kubia-ingress.yaml 
ingress.extensions/kubia-ingress created
[root@localhost ~]# kubectl get ingresses
NAME            HOSTS           ADDRESS   PORTS   AGE
kubia-ingress   kubia.exp.com             80      59s
[root@k8s-master ~]# kubectl describe ingress kubia-ingress
Name:             kubia-ingress
Namespace:        default
Address:          
Default backend:  default-http-backend:80 (<none>)
Rules:
  Host           Path  Backends
  ----           ----  --------
  kubia.exp.com  
                 /   kubia:80 (10.244.1.28:8080,10.244.2.43:8080,10.244.2.44:8080)
Annotations:
Events:
  Type    Reason  Age    From                      Message
  ----    ------  ----   ----                      -------
  Normal  CREATE  2m22s  nginx-ingress-controller  Ingress default/kubia-ingress
```

通过浏览器访问kubia.exp.com:30112即可访问到kubia服务.

客户端和控制器之间通信通过https加密,而控制器和后端pod之间通信则不是,运行在pod上的应用不需要支持TLS.例如pod运行web服务器,他只能接收http通信,并让Ingress控制器负责处理与TLS相关的所有内容,只需要将证书和私钥附加到Ingress,这两个资源储存在Secret资源中,然后在Ingress manifest中引用它:

```sh
[root@k8s-master ~]# openssl genrsa -out tls.key 2048
[root@k8s-master ~]# openssl req -new -x509 -key tls.key -out tls.crt -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=kubia.exp.com
[root@k8s-master ~]# kubectl create secret tls kubia-secret --cert=tls.crt --key=tls.key
[root@k8s-master ~]# kubectl get secret
NAME                    TYPE                                  DATA   AGE
default-token-5wkq2     kubernetes.io/service-account-token   3      25h
kubia-secret            kubernetes.io/tls                     2      30s
tomcat-ingress-secret   kubernetes.io/tls                     2      55m
```

重新创建ingress,使用https协议绑定kubia:

```sh
[root@k8s-master ~]# vi kubia-tls.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-kubia
  namespace: default
spec:
  tls:
  - hosts:
    - kubia.exp.com
    secretName: kubia-secret
  rules:
  - host: kubia.exp.com
    http:
      paths:
      - path:
        backend:
          serviceName: kubia
          servicePort: 8080
[root@k8s-master ~]# kubectl create -f kubia-tls.yaml 
ingress.extensions/ingress-kubia created
[root@k8s-master ~]# kubectl get ingress
NAME                 HOSTS              ADDRESS   PORTS     AGE
ingress-kubia        kubia.exp.com                80, 443   29s
ingress-myapp        myapp.along.com              80        65m
ingress-tomcat       tomcat.along.com             80        60m
ingress-tomcat-tls   tomcat.along.com             80, 443   58m
kubia-ingress        kubia.exp.com                80        10m
```

使用浏览器访问HTTPS地址: https://kubia.exp.com:30970/验证成功.

可以通过设置多个path转发访问路径到不同服务:

```sh
...
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia
          servicePort: 80
      - path: /admin
        backend:
          serviceName: admin
          servicePort: 80
```

同样可以设置多个rules来转发子域名到不同服务:

```sh
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia-ingress
  namespace: default
spec:
  rules:
  - host: kubia.exp.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia
          servicePort: 80
  rules:
  - host: app.exp.com
    http:
      paths:
      - path: /
        backend:
          serviceName: app
          servicePort: 80
      - path: /api
        backend:
          serviceName: api
          servicePort: 80
```

除了用NodePort,LoadBalancer等暴露单个服务,Ingress也能用来暴露服务:

```sh
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: my-ingress
spec:
  backend:
    serviceName: my-svc
    servicePort: 80
```

这样通过外网访问80端口便能直接转发到my-svc服务.



## 无头服务

假如客户需要获取一组pod IP,默认情况下查询服务会返回服务的集群IP.通过把服务IP设为None,客户端可以通过一个简单的DNS A记录查找获取到属于该服务的所有pod IP.这种服务叫headless服务.

### 创建服务

配置yaml并启动服务:

```sh
[root@server4-master ~]# vi kubia-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
  namespace: default
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
[root@server4-master ~]# kubectl create -f kubia-headless.yaml 
service/kubia-headless created
[root@server4-master ~]# kubectl get service
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
kubernetes       ClusterIP   10.96.0.1        <none>        443/TCP        130d
kubia            ClusterIP   10.107.244.106   <none>        80/TCP         3h43m
kubia-headless   ClusterIP   None             <none>        80/TCP         3s
kubia-nodeport   NodePort    10.99.116.227    <none>        80:30123/TCP   148m
```

创建完毕后,可以看到服务没有集群IP.

### 通过DNS查找

拉取一个能够DNS查询的容器后进入pod执行DNS查找:

```sh
[root@k8s-master ~]# kubectl run dnsutils --image=tutum/dnsutils --command -- sleep infinity
pod/dnsutils created
[root@k8s-master ~]# kubectl exec dnsutils nslookup kubia-headless
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   kubia-headless.default.svc.cluster.local
Address: 10.244.1.38
Name:   kubia-headless.default.svc.cluster.local
Address: 10.244.2.57
Name:   kubia-headless.default.svc.cluster.local
Address: 10.244.2.58
```

结果返回了pod的IP,这与普通服务不同,比如kubia服务返回服务的集群IP:

```sh
[root@k8s-master ~]# kubectl exec dnsutils nslookup kubia
Server:         10.96.0.10
Address:        10.96.0.10#53

Name:   kubia.default.svc.cluster.local
Address: 10.102.7.149
[root@k8s-master ~]# kubectl get svc
NAME             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes       ClusterIP   10.96.0.1      <none>        443/TCP   24h
kubia            ClusterIP   10.102.7.149   <none>        80/TCP    64s
```

headless服务使用上与普通服务一样,客户也可以通过连接服务的DNS名称来连接到pod上,不过客户端直连到pod上,而不是通过服务代理.

headless服务仍然提供跨pod的负载平衡,通过DNS轮询机制实现.

### 发现所有pod

如果希望pod没有准备就绪也能被发现,可以在服务配置中添加注解:

```sh
metadata:
  name: kubia-headless
  namespace: default
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
```

