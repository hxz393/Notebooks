# 安装配置

## Kubeadm

Kubeadm是由志愿者开发专门用于部署K8s的工具.在K8s 1.14之后,Kubeadm项目已经正式宣布GA(General Availability)了,可以在官方文档查看详细使用说明:https://kubernetes.io/zh/docs/setup/production-environment/tools/

### kubeadm init

初始化大体的过程如下:

1. kubeadm执行初始化检查.
2. 生成token和证书.
3. 生成KubeConfig文件,kubelet需要用这个文件与Master通信.
4. 安装Master组件,从镜像仓库下载组件的Docker镜像.
5. 安装附加组件kube-proxy和kube-dns.
6. Kubernetes Master初始化成功.
7. 提示如何配置kubectl,提示如何安装pod网络,提示如何注册到其他节点到Cluster.

执行kubeadm init指令后,kubeadm首先做一系列检查工作,以确定本机可以用来部署K8s.这一步检查称为Preflight Checks,它主要检查内核版本,CGroups模块,kubelet版本,网络端口等必要配置是否正确.

默认情况下与API服务器的通信必须使用HTTPS方式,因此需要证书文件.在通过系统检查后,kubeadm就开始生成K8s对外提供服务所需的各种证书和目录.kubeadm生成的证书存在主节点/etc/kubernetes/pki/目录下,最重要的文件是ca.crt和ca.key.也可以将现有证书复制到证书目录中,这样kubeadm不会生成证书:

```sh
[root@server4-master ~]# ls /etc/kubernetes/pki
apiserver.crt              apiserver.key                 ca.crt  front-proxy-ca.crt      front-proxy-client.key
apiserver-etcd-client.crt  apiserver-kubelet-client.crt  ca.key  front-proxy-ca.key      sa.key
apiserver-etcd-client.key  apiserver-kubelet-client.key  etcd    front-proxy-client.crt  sa.pub
```

证书生成后,kubeadm接下来会建立各种访问API服务器所需的配置文件,存放于/etc/kubernetes/目录下.这些文件记录了主节点的服务器地址端口和证书位置等信息,被对应的客户端直接使用:

```sh
[root@server4-master ~]# ls /etc/kubernetes/
admin.conf  controller-manager.conf  kubeadm-master.config  kubelet.conf  manifests  pki  scheduler.conf
```

接着kubeadm会为Master组件生成Pod配置文件,并以Static Pod的形式运行.这种容器启动方法允许将要部署的Pod的YAML文件放在一起,当Kubelet启动时,自动加载文件并启动Pod.默认存放在/etc/kubernetes/manifests/目录下面:

```sh
[root@server4-master ~]# ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

然后kubeadm会为集群生成一个bootstrap token,用来加入集群时使用.其他证书等信息通过ConfigMap的方式保存在etcd中.

最后是安装默认插件,包括kebe-proxy和DNS插件,用来提供整个集群的服务发现和DNS功能.

### kubeadm join

加入集群时,需要使用主节点上初始化时生成的token,也就是用来进行一次性的身份验证.在工作节点拿到了ConfigMap中的证书后,将使用证书进行安全通信.



## 准备工作

首先需要先安装好Docker,系统版本为CentOS7.

### 修改主机名

修改所有节点上的主机名,例如server4-master, server5-node1, server6-node2:

```sh
[root@server6 ~]# hostnamectl set-hostname server6-node2
```

将主机名添加到每台主机的/etc/hosts中:

```sh
[root@server6-node2 ~]# tee -a /etc/hosts <<-'EOF'
192.168.2.204 server4-master
192.168.2.205 server5-node1
192.168.2.206 server6-node2
EOF
```

### 禁用虚拟内存

```sh
[root@server4-master ~]# swapoff -a
[root@server4-master ~]# mount -n -o remount,rw /
[root@server4-master ~]# sed -i 's/.*swap.*/#&/' /etc/fstab
```

### 开启端口转发

首先要确认网卡MAC地址没有冲突,然后确保br_netfilter模块被加载:

```sh
[root@server6-node2 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0 
bridge                151336  1 br_netfilter
[root@server4-master ~]# sysctl net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-iptables = 1
[root@server4-master ~]# modprobe ip_vs
[root@server4-master ~]# modprobe ip_vs_rr
[root@server4-master ~]# modprobe ip_vs_wrr
[root@server4-master ~]# modprobe ip_vs_sh
[root@server4-master ~]# cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
[root@server6-node2 ~]# sysctl --system
```

### 修改SELinux设置

将SELinux设置为被动模式,否则容器访问主机文件系统会不正常:

```sh
[root@server4-master ~]# setenforce 0
[root@server4-master ~]# sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
```



## 安装工具

kubelet是唯一没有以容器形式运行的kubernetes组件,它通过systemd服务运行.kubeadm用来创建K8s集群,kubectl是用来执行K8s命令的工具.其他Kubernetes的系统组件都以容器化运行,并被放到kube-system namespaces中,例如coredns, etcd, apiserver, controller-manager.

### 安装

在所有节点上安装kubelet,kubeadm和kubectl:

```sh
[root@server5-node1 ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF
[root@server4-master ~]# yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
[root@server4-master ~]# systemctl enable --now kubelet
```

### 启动Kubelet

先修改Kubelet配置文件再启动:

```sh
[root@server4-master ~]# cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS="--cgroup-driver=cgroupfs --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2"
EOF
[root@server4-master ~]# systemctl daemon-reload
[root@server4-master ~]# systemctl enable kubelet && systemctl restart kubelet
```



## 建立集群

### 初始化Master

可以生成默认kubeadm-master.config配置文件,修改后指定使用配置运行:

```sh
[root@server4-master ~]# kubeadm config print init-defaults > /etc/kubernetes/kubeadm-master.config
```

提前拉取镜像,如果执行失败可以多次执行:

```sh
[root@server4-master ~]# kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers
```

使用参数指定初始化:

```sh
[root@server4-master ~]# kubeadm init --apiserver-advertise-address=192.168.2.204 --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.22.0 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.2.204:6443 --token 0tado4.y6izl96qxbtck0rw \
        --discovery-token-ca-cert-hash sha256:8055ad40e280bc4b48b0c3b4daea9aa3f515d3b2fea5fb3ae3fcad398c325a52
```

主要需要的五个参数:

- --apiserver-advertise-address: 设置本机的IP地址.
- --image-repository: 设置镜像仓库地址.
- --kubernetes-version: 设置K8s版本
- --service-cidr: 设置服务器通信使用的IP网段.
- --pod-network-cidr: 设置内部的pod节点之间网络可以使用的IP段.

### 重置节点

初始化失败可以重置并删除文件:

```sh
[root@k8s-master ~]# kubeadm reset
[reset] Reading configuration from the cluster...
...
[root@k8s-master ~]# rm -rf /var/lib/cni/ /etc/cni/net.d $HOME/.kube/config /var/lib/etcd
```

### 配置Kubectl

按照提示,将变量声明写入到.bashrc中:

```sh
[root@server4-master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >>~/.bashrc
```

配置Kubectl命令自动补全:

```sh
[root@server4-master ~]# echo "source <(kubectl completion bash)" >>~/.bashrc
[root@server4-master ~]# source ~/.bashrc
```

### 安装网络

可以选择Flannel或Calico这种高性能Underlay网络.只能安装一种网络.安装Flannel网络方式如下:

```sh
[root@server4-master ~]# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
podsecuritypolicy.policy/psp.flannel.unprivileged created
clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.apps/kube-flannel-ds created
```

安装Calico网络方式如下:

```sh
[root@server4-master ~]# kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
[root@server4-master ~]# kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
installation.operator.tigera.io/default created
```

如果初始化时设置不是--pod-network-cidr=192.168.0.0/16,需要修改yaml文件中CIDR值:

```sh
[root@server4-master ~]# wget https://docs.projectcalico.org/manifests/custom-resources.yaml
[root@server4-master ~]# vi custom-resources.yaml 
spec:
  # Configures Calico networking.
  calicoNetwork:
    # Note: The ipPools section cannot be modified post-install.
    ipPools:
    - blockSize: 26
      cidr: 10.244.0.0/16
      encapsulation: VXLANCrossSubnet
      natOutgoing: Enabled
      nodeSelector: all()
[root@server4-master ~]# kubectl create -f custom-resources.yaml 
[root@server4-master ~]# watch kubectl get pods -n calico-system
```

### 添加Node

可以在节点上生成添加节点的配置文件来使用:

```sh
[root@server5-node1 ~]# kubeadm config print join-defaults > kubeadm-config.yaml
```

也可以不使用配置,通过指定Master初始化后输出的token来执行加入命令:

```sh
[root@server6-node2 ~]# kubeadm join 192.168.2.204:6443 --token 0tado4.y6izl96qxbtck0rw --discovery-token-ca-cert-hash sha256:8055ad40e280bc4b48b0c3b4daea9aa3f515d3b2fea5fb3ae3fcad398c325a52
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

查看已加入节点:

```sh
[root@server4-master ~]# kubectl get nodes
NAME             STATUS     ROLES                  AGE   VERSION
server4-master   Ready      control-plane,master   31m   v1.22.3
server5-node1    Ready      <none>                 84s   v1.22.3
server6-node2    NotReady   <none>                 19s   v1.22.3
```

通过--all-namespaces参数来查看所有节点容器运行信息:

```sh
[root@server4-master ~]# kubectl get pod --all-namespaces -o wide
NAMESPACE   NAME                                          READY  STATUS    RESTARTS   AGE
kube-system coredns-5c98db65d4-46tmf                      1/1    Running   0          158m
kube-system coredns-5c98db65d4-f8j8p                      1/1   Running   0          158m
kube-system etcd-localhost.localdomain                    1/1   Running   0          48m
kube-system kube-apiserver-localhost.localdomain          1/1   Running   0          48m
kube-system kube-controller-manager-localhost.localdomain 1/1   Running   0          48m
kube-system kube-flannel-ds-amd64-2h75p                   1/1   Running   0          83s
kube-system kube-flannel-ds-amd64-flr7r                   1/1   Running   0          72m
kube-system kube-flannel-ds-amd64-m62pz                   1/1   Running   0          89s
kube-system kube-proxy-jzrxp                              1/1   Running   0          83s
kube-system kube-proxy-ljrj8                              1/1   Running   0          89s
kube-system kube-proxy-q5clk                              1/1   Running   0          158m
kube-system kube-scheduler-localhost.localdomain          1/1   Running   0          48m
```



## K8s设置

### 开放权限

绑定集群管理员角色:

```sh
[root@k8s-master ~]# kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts
clusterrolebinding.rbac.authorization.k8s.io/permissive-binding created
```

删除绑定:

```sh
[root@k8s-master 2]# kubectl delete clusterrolebinding permissive-binding
clusterrolebinding.rbac.authorization.k8s.io "permissive-binding" deleted
```

### 允许Master节点部署

删除Master上面的污点:

```sh
[root@k8s-master pv]# kubectl taint nodes k8s-master node-role.kubernetes.io/master=true:NoSchedule-
```

重新添加污点:

```sh
[root@k8s-master pv]# kubectl taint nodes k8s-master node-role.kubernetes.io/master=true:NoSchedule
node/k8s-master tainted
```



## 插件安装

主要是各种可以通过浏览器来可视化管理k8s的项目.

### Dashboard

K8s的WebUI需要额外部署:

```sh
[root@k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml
```

修改node为NodePort模式:

```sh
[root@k8s-master ~]# kubectl patch svc -n kubernetes-dashboard kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}'
service/kubernetes-dashboard patched
```

查看NodePort端口:

```sh
[root@server4-master ~]# kubectl get svc --all-namespaces
NAMESPACE              NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
kubernetes-dashboard   kubernetes-dashboard              NodePort    10.107.244.106   <none>        443:31163/TCP            21s
```

还需要创建token才可以通过浏览器访问31163端口:

```sh
[root@k8s-master 10]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
serviceaccount/dashboard-admin created
[root@k8s-master 10]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
clusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created
[root@k8s-master 10]# kubectl describe secret -n kubernetes-dashboard dashboard-admin-token
Name:         dashboard-admin-token-nrm6q
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: 5f0cc597-e403-47af-b3b1-25de06c82723

Type:  kubernetes.io/service-account-token

Data
====
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbnJtNnEiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNWYwY2M1OTctZTQwMy00N2FmLWIzYjEtMjVkZTA2YzgyNzIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.uBjaS6PqTJTJvK9B9M8yNp--BpAdLzOpzFiqfgkd1GIPI3haOSQMmLufE-XIkbPhz4NLmmyJ5z4Jhini7WJgKTn8p9CR2M_yhlaPhOHaZl3WqjEAp2AwU98PFIjlNgbRv7U1vEt1HEL0mJ6HPLo8fqHa4AziwlwYm_uTqmk5sVm9V0YisRs-21V5IohLfpEWIBfU2ddpdqzZV3qkgEbVYGo3GQ3CBKjDilJh-DtMvejpJRUSetKxL97VLmYXwHLvrkqZJzvccCBkAreXKbXpJVTzrs9ya67RTqalw_tWxFlB--ASLRcmoEMxdfJ406G-YF5NEDK20jIvK6HitSurqQ
ca.crt:     1025 bytes
```

### Rook

Rook是一个基于Ceph的K8s储存插件,同样可以使用容器直接部署:

```sh
[root@server4-master ~]# git clone https://github.com/rook/rook.git
[root@server4-master ~]# cd rook/cluster/examples/kubernetes/ceph
```

但由于k8s.gcr.io访问不了,所以需要修改配置文件:

```sh
[root@server4-master ceph]# kubectl delete -f crds.yaml -f common.yaml -f operator.yaml -f cluster.yaml
[root@server4-master ceph]# vi operator.yaml
ROOK_CSI_CEPH_IMAGE: "quay.io/cephcsi/cephcsi:v3.1.2"
ROOK_CSI_REGISTRAR_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-node-driver-registrar:v2.0.1"
ROOK_CSI_RESIZER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-resizer:v1.0.0"
ROOK_CSI_PROVISIONER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-provisioner:v2.0.0"
ROOK_CSI_SNAPSHOTTER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-snapshotter:v3.0.0"
ROOK_CSI_ATTACHER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-attacher:v3.0.0"
[root@server4-master ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml
[root@server4-master ceph]# kubectl create -f cluster.yaml
```



### 其他项目

其他第三方管理工具还有[Kuboard](https://kuboard.cn/)和[KubeSphere](https://kubesphere.io/)





## 故障处理

### Flannel错误处理

Flannel报错提示没有分配cidr.这是由于使用kubeadm初始化时没有指定--pod-network-cidr参数所致:

```sh
[root@server4-master ~]# kubectl logs -n kube-system kube-flannel-ds-7xp24 
I1103 04:42:22.796211       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
E1103 04:42:22.797028       1 main.go:325] Error registering network: failed to acquire lease: node "server4-master" pod cidr not assigned
I1103 04:42:22.797248       1 main.go:439] Stopping shutdownHandler...
```

修改配置文件kube-controller-manager.yaml,添加- --allocate-node-cidrs=true和- --cluster-cidr=10.244.0.0/16:

```sh
[root@server4-master ~]# vi /etc/kubernetes/manifests/kube-controller-manager.yaml
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --cluster-cidr=10.244.0.0/16
```

删除错误的flannel容器,重建之后就能使用正确的配置了:

```sh
[root@server4-master ~]# kubectl delete pod -n kube-system kube-flannel-*
[root@server4-master ~]# kubectl cluster-info dump | grep -m 1 cluster-cidr
                            "--cluster-cidr=10.244.0.0/16",
```

### Token过期

加入时报错x509: certificate has expired or is not yet valid,可以重新生成token:

```sh
[root@k8s-master ~]# kubeadm token create
87668a.lfvdqsr3l5ijlode
[root@k8s-master ~]# kubeadm token list
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
1yrznt.lzj1kl0qecosl4es   23h       2019-07-21T13:14:33+08:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token
87668a.lfvdqsr3l5ijlode   23h       2019-07-21T13:24:28+08:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token
[root@server7-master ~]# kubeadm init phase upload-certs --upload-certs
```

在客户端同步时间,再次加入成功:

```
[root@k8s-node1 ~]# ntpdate cn.pool.ntp.org
20 Jul 13:25:59 ntpdate[118642]: step time server 199.182.204.197 offset 137559.272548 sec
```



## 建立高可用集群

由三个master组成主节点集群,通过内网loader balancer实现负载均衡.至少需要三个master节点才可组成高可用集群,否则会出现脑裂现象.

多个Node组成工作节点集群,通过外网loader balancer实现负载均衡.

### 环境设置

下面命令在所有主机上都要运行.先设置好本地环境:

```sh
[root@server7-master ~]# echo "127.0.0.1   $(hostname)" >> /etc/hosts
[root@server7-master ~]# systemctl stop firewalld
[root@server7-master ~]# systemctl disable firewalld
[root@server7-master ~]# setenforce 0
[root@server7-master ~]# sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
[root@server9-node1 ~]# swapoff -a
yes | cp /etc/fstab /etc/fstab_bak
[root@server9-node1 ~]# yes | cp /etc/fstab /etc/fstab_bak
[root@server9-node1 ~]# cat /etc/fstab_bak |grep -v swap > /etc/fstab
[root@server7-master ~]# echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
[root@server7-master ~]# echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf
[root@server7-master ~]# echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf
[root@server7-master ~]# sysctl -p
[root@server7-master ~]# sed -i "s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g" /usr/lib/systemd/system/docker.service
[root@server7-master ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io
docker version >= 1.12
{"registry-mirrors": ["http://f1361db2.m.daocloud.io"]}
Success.
You need to restart docker to take effect: sudo systemctl restart docker 
```

安装Kubelet:

```sh
[root@server9-node1 ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
[root@server7-master ~]# yum install -y kubelet-1.16.2 kubeadm-1.16.2 kubectl-1.16.2
[root@server7-master ~]# systemctl daemon-reload
[root@server7-master ~]# systemctl restart docker
[root@server7-master ~]# systemctl enable kubelet && systemctl start kubelet
```

### 建立集群

初始化第一个Master主节点:

```sh
[root@server7-master ~]# export APISERVER_NAME=master1
[root@server7-master ~]# export POD_SUBNET=10.100.0.1/16
[root@server7-master ~]# echo "127.0.0.1    ${APISERVER_NAME}" >> /etc/hosts
[root@server7-master ~]# cat <<EOF > ./kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.16.2
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "${APISERVER_NAME}:6443"
networking:
  serviceSubnet: "10.96.0.0/16"
  podSubnet: "${POD_SUBNET}"
  dnsDomain: "cluster.local"
EOF
[root@server7-master ~]# kubeadm init --config=kubeadm-config.yaml --upload-certs
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join master1:6443 --token 6taat7.7rh4xocsrw3wahxv \
    --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d \
    --control-plane --certificate-key 7cead00cc2f6e54175d62e97ab50a940d3dc323ff44256558134fdf11536b7d0

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use 
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join master1:6443 --token 6taat7.7rh4xocsrw3wahxv \
    --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d 
[root@server7-master ~]# mkdir -p $HOME/.kube && cp -i /etc/kubernetes/admin.conf /root/.kube/config
[root@server7-master ~]# wget https://kuboard.cn/install-script/calico/calico-3.9.2.yaml
[root@server7-master ~]# sed -i "s#192\.168\.0\.0/16#${POD_SUBNET}#" calico-3.9.2.yaml
[root@server7-master ~]# kubectl apply -f calico-3.9.2.yaml
```

等待网络部署好,所有九个容器部署完毕:

```sh
[root@server7-master ~]# watch kubectl get pod -n kube-system -o wide
```

初始化第二和第三个Master节点:

```sh
[root@server8-node1 ~]# export APISERVER_IP=192.168.2.207
[root@server8-node1 ~]# export APISERVER_NAME=master1
[root@server8-node1 ~]# echo "${APISERVER_IP}    ${APISERVER_NAME}" >> /etc/hosts
[root@server8-node1 ~]# kubeadm join master1:6443 --token 6taat7.7rh4xocsrw3wahxv \
     --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d \
     --control-plane --certificate-key 7cead00cc2f6e54175d62e97ab50a940d3dc323ff44256558134fdf11536b7d0
```

工作节点加入命令:

```sh
[root@server8-node1 ~]# export APISERVER_IP=192.168.2.207
[root@server8-node1 ~]# export APISERVER_NAME=master1
[root@server8-node1 ~]# echo "${APISERVER_IP}    ${APISERVER_NAME}" >> /etc/hosts
[root@server8-node1 ~]# kubeadm join master1:6443 --token 6taat7.7rh4xocsrw3wahxv \
    --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d 
```

移除节点在要移除的节点上运行kubeadm rest命令:

```sh
[root@server9-node1 ~]# kubeadm reset
```

在主节点上运行kubectl delete node命令:

```sh
[root@server7-master ~]# kubectl delete node server9-node1
```

另可参考: https://github.com/fanux/sealos

