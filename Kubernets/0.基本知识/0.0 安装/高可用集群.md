# 搭建高可用集群

由三个master组成主节点集群,通过内网loader balancer实现负载均衡.至少需要三个master节点才可组成高可用集群,否则会出现脑裂现象.

多个Node组成工作节点集群,通过外网loader balancer实现负载均衡.



## 环境设置

下面命令在所有主机上都要运行.先设置好本地环境:

```sh
[root@server7-master ~]# echo "127.0.0.1   $(hostname)" >> /etc/hosts
[root@server7-master ~]# systemctl stop firewalld
[root@server7-master ~]# systemctl disable firewalld
[root@server7-master ~]# setenforce 0
[root@server7-master ~]# sed -i "s/SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/config
[root@server7-master ~]# swapoff -a
[root@server7-master ~]# yes | cp /etc/fstab /etc/fstab_bak
[root@server7-master ~]# cat /etc/fstab_bak |grep -v swap > /etc/fstab
[root@server7-master ~]# echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.conf
[root@server7-master ~]# echo "vm.swappiness = 0" >> /etc/sysctl.conf
[root@server7-master ~]# echo "net.ipv4.ip_nonlocal_bind = 1" >> /etc/sysctl.conf
[root@server7-master ~]# echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf
[root@server7-master ~]# echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf
[root@server7-master ~]# sysctl -p
[root@server7-master ~]# sed -i "s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g" /usr/lib/systemd/system/docker.service
[root@server7-master ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io
```

安装Kubelet和kubeadm:

```sh
[root@server7-master ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
[root@server7-master ~]# yum install -y kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3
[root@server7-master ~]# systemctl daemon-reload
[root@server7-master ~]# systemctl restart docker
[root@server7-master ~]# systemctl enable kubelet && systemctl start kubelet
```



## 部署keepalived

keepalived是以VRRP(虚拟路由冗余协议)协议为基础,包括一个master和多个backup.master劫持vip对外提供服务.master发送组播,backup节点收不到vrrp包时认为master宕机,此时选出剩余优先级最高的节点作为新master劫持vip.

此处的keeplived的主要作用是为haproxy提供vip(192.168.2.199).在三个haproxy实例之间提供主备,降低当其中一个haproxy失效的时对服务的影响.

先在所有主节点上安装keepalived:

```sh
[root@server7-master ~]# yum -y install keepalived psmisc
```

修改主节点上配置文件:

```sh
[root@server7-master ~]# vi /etc/keepalived/keepalived.conf
global_defs {
    router_id LVS_DEVEL
    script_user root
    enable_script_security
}
vrrp_script check_haproxy {
script "/usr/bin/killall -0 haproxy"
interval 3
weight -11
}
vrrp_instance VI_1 {
    state MASTER
    interface ens37
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.2.199
    }
track_script {
check_haproxy
}
}
```

其他节点的配置只用修改state和priority属性值:

```sh
[root@server8-node1 ~]# vi /etc/keepalived/keepalived.conf
global_defs {
    router_id LVS_DEVEL
    script_user root
    enable_script_security
}
vrrp_script check_haproxy {
script "/usr/bin/killall -0 haproxy"
interval 3
weight -11
}
vrrp_instance VI_1 {
    state BACKUP
    interface ens37
    virtual_router_id 51
    priority 90
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.2.199
    }
track_script {
check_haproxy
}
}
```

配置文件中主要配置项有:

- vrrp_script check_haproxy{}: 检测haproxy进程是否存活脚本,interval设置检测间隔为3秒.如果使用Docker运行,可以设置脚本为`script "/bin/bash -c 'if [[ $(netstat -nlp | grep 9443) ]]; then exit 0; else exit 1; fi'"`来监听haproxy运行端口.
- state MASTER: 指定节点名称.
- virtual_router_id 51: 虚拟路由组的id.
- interface ens37: 指定vip绑定网卡名称.
- priority 100: 节点优先级.
- authentication: 节点之间认证密码.
- virtual_ipaddress{}: 虚拟vip的ip地址.

在所有节点上启动服务:

```sh
[root@server7-master ~]# systemctl enable keepalived.service
systemctl start keepalived.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.
[root@server7-master ~]# systemctl start keepalived.service
```



## 部署HAproxy

haproxy提供高可用性,负载均衡,基于TCP和HTTP的代理,支持数以万记的并发连接.

此处haproxy为apiserver提供反向代理,haproxy将所有请求轮询转发到每个master节点上.

在所有节点上安装并配置haproxy,配置文件内容相同,主要是设置backend kubernetes-apiserver中server的地址列表,负载均衡模式balance默认为轮询的负载算法roundrobin:

```bash
[root@server7-master ~]# yum install -y haproxy
[root@server7-master ~]# vi /etc/haproxy/haproxy.cfg 
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# kubernetes apiserver frontend which proxys to the backends
#---------------------------------------------------------------------
frontend kubernetes-apiserver
    mode                 tcp
    bind                 *:16443
    option               tcplog
    default_backend      kubernetes-apiserver

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server  server7-master 192.168.2.207:6443 check
    server  server8-node1 192.168.2.208:6443 check
    server  server9-node2 192.168.2.209:6443 check

#---------------------------------------------------------------------
# collection haproxy statistics message
#---------------------------------------------------------------------
listen stats
    bind                 *:1080
    stats auth           admin:awesomePassword
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            /admin?stats
```

启动后观察端口监听状态,16443端口绑定服务监听,1080端口为状态查询前端入口,可以访问http://192.168.2.208:1080/admin?stats来查询haproxy运行状态:

```sh
[root@server7-master ~]# systemctl start haproxy.service
systemctl enable haproxy.service
[root@server7-master ~]# systemctl enable haproxy.service
Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
[root@server7-master ~]# systemctl status haproxy
[root@server7-master ~]# netstat -ntulp |grep 16443
tcp        0      0 0.0.0.0:16443           0.0.0.0:*               LISTEN      122579/haproxy      
[root@server7-master ~]# netstat -ntulp |grep 1080
tcp        0      0 0.0.0.0:1080            0.0.0.0:*               LISTEN      122579/haproxy   
```

使用ip a s查看当前vip绑定的主机,并在停止绑定主机上停止keepalived或haproxy服务后,观察vip的漂移:

```sh
[root@server7-master ~]# ip a s
2: ens37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:f6:ab:84 brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.207/24 brd 192.168.2.255 scope global noprefixroute ens37
       valid_lft forever preferred_lft forever
    inet 192.168.2.199/32 scope global ens37
       valid_lft forever preferred_lft forever
[root@server7-master ~]# systemctl stop keepalived
[root@server8-node1 ~]# ip a s
2: ens37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:10:4d:fd brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.208/24 brd 192.168.2.255 scope global noprefixroute ens37
       valid_lft forever preferred_lft forever
    inet 192.168.2.199/32 scope global ens37
       valid_lft forever preferred_lft forever
```

vip会根据配置文件中设置的优先级来选择主机.



## 建立集群

初始化第一个Master主节点:

```sh
[root@server7-master ~]# export APISERVER_NAME=vip
[root@server7-master ~]# export VIP_IP=192.168.2.199
[root@server7-master ~]# export P_SUBNET=10.100.0.1/16
[root@server7-master ~]# export K8S_VERSION=v1.22.3
[root@server7-master ~]# echo "${VIP_IP}    ${APISERVER_NAME}" >> /etc/hosts
[root@server7-master ~]# cat <<EOF > ./kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: "${K8S_VERSION}"
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "${APISERVER_NAME}:6443"
networking:
  serviceSubnet: "10.96.0.0/16"
  podSubnet: "${P_SUBNET}"
  dnsDomain: "cluster.local"
EOF
[root@server7-master ~]# kubeadm init --config=kubeadm-config.yaml --upload-certs

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join vip:6443 --token o5v6fg.fml7267hnzagm7g3 \
        --discovery-token-ca-cert-hash sha256:ac4d2d0fbd1eed34475be6fca421a3b232b8002d7012a1e0b5e336b18b78a5db \
        --control-plane --certificate-key 1f289aec22dce2984c60252d9ff00e1e7b5e9c02f422ddd34f5aa915cce53932

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join vip:6443 --token o5v6fg.fml7267hnzagm7g3 \
        --discovery-token-ca-cert-hash sha256:ac4d2d0fbd1eed34475be6fca421a3b232b8002d7012a1e0b5e336b18b78a5db 
[root@server7-master ~]# mkdir -p $HOME/.kube && cp -i /etc/kubernetes/admin.conf /root/.kube/config
[root@server4-master ~]# echo "source <(kubectl completion bash)" >>~/.bashrc
[root@server4-master ~]# source ~/.bashrc
```

部署网络,可以是flannel或calico:

```sh
[root@server4-master ~]# kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
[root@server7-master ~]# wget https://kuboard.cn/install-script/calico/calico-3.9.2.yaml
[root@server7-master ~]# sed -i "s#192\.168\.0\.0/16#${pod_SUBNET}#" calico-3.9.2.yaml
[root@server7-master ~]# kubectl apply -f calico-3.9.2.yaml
```

等待网络部署好,所有8个容器部署完毕:

```sh
[root@server7-master ~]# kubectl get pod -n kube-system -o wide --watch
```

初始化第二和第三个Master节点:

```sh
[root@server8-node1 ~]# export VIP_IP=192.168.2.199
[root@server8-node1 ~]# export APISERVER_NAME=vip
[root@server8-node1 ~]# echo "${VIP_IP}    ${APISERVER_NAME}" >> /etc/hosts
[root@server8-node1 ~]# kubeadm join ${VIP_IP}:6443 --token 6taat7.7rh4xocsrw3wahxv \
     --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d \
     --control-plane --certificate-key 7cead00cc2f6e54175d62e97ab50a940d3dc323ff44256558134fdf11536b7d0
```

工作节点加入命令:

```sh
[root@server8-node1 ~]# export VIP_IP=192.168.2.199
[root@server8-node1 ~]# export APISERVER_NAME=vip
[root@server8-node1 ~]# echo "${VIP_IP}    ${APISERVER_NAME}" >> /etc/hosts
[root@server8-node1 ~]# kubeadm join ${VIP_IP}:6443 --token 6taat7.7rh4xocsrw3wahxv \
    --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d 
```

查看etcd集群状态:

```sh
[root@server8-node1 ~]# kubectl exec -ti -n kube-system etcd-server8-node1 -- sh
sh-5.0# export ETCDCTL_API=3
sh-5.0# <lthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list
273040abda86d599, started, server9-node1, https://192.168.2.209:2380, https://192.168.2.209:2379, false
4c80ad008d949110, started, server8-node1, https://192.168.2.208:2380, https://192.168.2.208:2379, false
dcddbc08262b69e1, started, server7-master, https://192.168.2.207:2380, https://192.168.2.207:2379, false
```

移除节点在要移除的节点上运行kubeadm rest命令:

```sh
[root@server9-node1 ~]# kubeadm reset
[root@server9-node1 ~]# rm -rf /var/lib/cni/ /etc/cni/net.d $HOME/.kube/config /var/lib/etcd
[root@server7-master ~]# ifconfig cni0 down
[root@server7-master ~]# ip link delete cni0
```

在主节点上运行kubectl delete node命令:

```sh
[root@server7-master ~]# kubectl delete node server9-node1
```

另可参考: https://github.com/fanux/sealos



## 加入集群

要加入集群时需要--discovery-token-ca-cert-hash参数,可用下面命令获得:

```sh
[root@server4-master ~]# kubeadm token create
[root@server4-master ~]# kubeadm token list
[root@server4-master ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
8055ad40e280bc4b48b0c3b4daea9aa3f515d3b2fea5fb3ae3fcad398c325a52
```

如果是作为主节点加入,需要另外一个另外一个--certificate-key参数,可以重新生成:

```sh
[root@server4-master ~]# kubeadm init phase upload-certs --upload-certs
I0329 13:23:45.461510  129111 version.go:255] remote version is much newer: v1.23.5; falling back to: stable-1.22
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
5ac22cea4cfc19753cd289c96b968957554b4e4204fe47fc3f56a53dded96716
```

如果想作为第二个主节点,加入一个已经存在的主节点中,在preflight阶段会报错,原因是controlPlane没有Endpoint:

```sh
[root@server1 ~]# kubeadm join 192.168.2.204:6443 --token nre8hl.4awo2mnyxr5l0tsu      --discovery-token-ca-cert-hash sha256:8055ad40e280bc4b48b0c3b4daea9aa3f515d3b2fea5fb3ae3fcad398c325a52      --control-plane --certificate-key fd44cac1bee0d209dc8e0b7f0a68f9e43e43aef4de05fccf7c7c4a5baac01889
error execution phase preflight: 
One or more conditions for hosting a new control plane instance is not satisfied.

unable to add a new control plane instance a cluster that doesn't have a stable controlPlaneEndpoint address

Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.
```

使用kubectl edit命令编辑kubeadm-config配置文件,添加controlPlaneEndpoint地址为vip或运行中的主节点IP:

```sh
[root@server4-master ~]# kubectl edit cm kubeadm-config -n kube-system
    imageRepository: registry.aliyuncs.com/google_containers
    kind: ClusterConfiguration
    controlPlaneEndpoint: 192.168.2.204:6443
    kubernetesVersion: v1.22.3
    networking:
      dnsDomain: cluster.local
      podSubnet: 10.244.0.0/16
```

之后节点通过kubeadm join即可加入.



## 阿里云部署

在阿里云上部署K8s时ecs不支持自建vip,指定slb的vip为高可用地址时初始化master会失败.这是由于4层SLB不支持其调度的后端服务器访问其vip,即服务器不能又当服务端又做客户端.

解决办法是在hosts文件里解析个域名作为k8s api地址,master的hosts文件把127.0.0.1指向此域名,node的hosts正常解析此域名为slb地址即可.

参考网址:

https://www.cnblogs.com/gmmy/p/12372805.html

https://www.cnblogs.com/noah-luo/p/13218837.html

https://zhuanlan.zhihu.com/p/107703112

https://blog.51cto.com/caiyuanji/2405434

https://blog.csdn.net/chenleiking/article/details/84841394

https://jimmysong.io/kubernetes-handbook/

https://www.jianshu.com/p/ba94db1c3599

https://blog.csdn.net/weixin_44334279/article/details/119355653
