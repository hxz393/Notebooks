# 安装配置

## Kubeadm

Kubeadm是由志愿者开发专门用于部署K8s的工具.在K8s 1.14之后,Kubeadm项目已经正式宣布GA(General Availability)了,可以在官方文档查看详细使用说明: https://kubernetes.io/zh/docs/setup/production-environment/tools/

### kubeadm init

初始化大体的过程如下:

1. kubeadm执行初始化检查.
2. 生成token和证书.
3. 生成KubeConfig文件,kubelet需要用这个文件与Master通信.
4. 安装Master组件,从镜像仓库下载组件和Docker镜像.
5. 安装附加组件kube-proxy和kube-dns.
6. Kubernetes Master初始化成功.
7. 提示如何配置kubectl,安装pod网络,其他节点注册到集群.

执行kubeadm init指令后,kubeadm首先做一系列检查工作,以确定本机可以用来部署K8s.这一步检查称为Preflight Checks,它主要检查内核版本,CGroups模块,kubelet版本,网络端口等必要配置是否正确.

默认情况下与API服务器的通信必须使用HTTPS方式,因此需要证书文件.在通过系统检查后,kubeadm就开始生成K8s对外提供服务所需的各种证书和目录.kubeadm生成的证书存在主节点/etc/kubernetes/pki/目录下,最重要的文件是ca.crt和ca.key.也可以将现有证书复制到证书目录中,这样kubeadm不会生成证书:

```sh
[root@k8s-250 ~]# ls /etc/kubernetes/pki
apiserver.crt              apiserver.key                 ca.crt  front-proxy-ca.crt      front-proxy-client.key
apiserver-etcd-client.crt  apiserver-kubelet-client.crt  ca.key  front-proxy-ca.key      sa.key
apiserver-etcd-client.key  apiserver-kubelet-client.key  etcd    front-proxy-client.crt  sa.pub
```

证书生成后,kubeadm接下来会建立各种访问API服务器所需的配置文件,存放于/etc/kubernetes/目录下.这些文件记录了主节点的服务器地址端口和证书位置等信息,被对应的客户端直接使用:

```sh
[root@k8s-250 ~]# ls /etc/kubernetes/
admin.conf  controller-manager.conf  kubeadm-master.config  kubelet.conf  manifests  pki  scheduler.conf
```

接着kubeadm会为Master组件生成pod配置文件,并以静态pod的形式运行.这种容器启动方法允许将要部署的pod的YAML文件放在一起,当Kubelet启动时,自动加载文件并启动pod.这些配置文件默认存放在/etc/kubernetes/manifests/目录下:

```sh
[root@k8s-250 ~]# ll /etc/kubernetes/manifests/
total 16
-rw------- 1 root root 2258 Apr 20 22:19 etcd.yaml
-rw------- 1 root root 3424 Apr 20 22:29 kube-apiserver.yaml
-rw------- 1 root root 2906 Apr 20 22:30 kube-controller-manager.yaml
-rw------- 1 root root 1492 Apr 20 22:30 kube-scheduler.yaml
```

然后kubeadm会为集群生成一个bootstrap token,用来加入集群时使用.其他证书等信息通过ConfigMap的方式保存在etcd中.

最后是安装默认插件,包括kebe-proxy和DNS插件,用来提供整个集群的服务发现和DNS功能.

### kubeadm join

加入集群时,需要使用主节点上初始化时生成的token,也就是用来进行一次性的身份验证.在工作节点拿到了ConfigMap中的证书后,将使用证书进行安全通信.



## 准备工作

首先需要先安装好Docker,系统版本为CentOS 7.

### 修改主机名

修改所有节点上的主机名,例如server4-master, server5-node1, server6-node2:

```sh
[root@server4 ~]# hostnamectl set-hostname k8s-204
```

将主机名添加到/etc/hosts中:

```sh
[root@server4 ~]# echo "127.0.0.1   $(hostname)" >> /etc/hosts
```

### 禁用虚拟内存

调整开机挂载选项,并重新挂载根目录:

```sh
[root@server4-master ~]# swapoff -a
[root@server7-master ~]# yes | cp /etc/fstab /etc/fstab_bak
[root@server7-master ~]# cat /etc/fstab_bak |grep -v swap > /etc/fstab
[root@server4-master ~]# mount -n -o remount,rw /
[root@server4-master ~]# sed -i 's/.*swap.*/#&/' /etc/fstab
```

### 开启端口转发

首先要确认网卡MAC地址没有冲突,然后确保br_netfilter模块被加载:

```sh
[root@server6-node2 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0 
bridge                151336  1 br_netfilter
[root@server4-master ~]# modprobe ip_vs
[root@server4-master ~]# modprobe ip_vs_rr
[root@server4-master ~]# modprobe ip_vs_wrr
[root@server4-master ~]# modprobe ip_vs_sh
[root@server4-master ~]# cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_nonlocal_bind = 1
net.ipv4.ip_forward = 1
vm.swappiness=0
EOF
[root@server6-node2 ~]# sysctl --system
```

### 关闭防火墙

关闭防火墙和SELinux,否则容器访问主机文件系统会不正常:

```sh
[root@k8s-204 ~]# service iptables stop
[root@k8s-204 ~]# chkconfig iptables off
[root@k8s-204 ~]# systemctl stop firewalld
[root@k8s-204 ~]# systemctl disable firewalld
[root@k8s-204 ~]# setenforce 0
[root@k8s-204 ~]# sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config
[root@k8s-204 ~]# getenforce #
```



## 安装组件

kubelet是唯一没有以容器形式运行的k8s组件,它通过systemd服务运行.kubeadm用来创建K8s集群,kubectl是用来执行K8s命令的工具.其他k8s的系统组件都以容器化运行,并被放到kube-system namespaces中,例如coredns, etcd, apiserver, controller-manager.

在所有要加入K8s集群的主机中安装kubelet, kubeadm和kubectl 1.22.3版本:

```sh
[root@server5-node1 ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
[root@server4-master ~]# yum install -y kubelet-1.22.3 kubeadm-1.22.3 kubectl-1.22.3 --disableexcludes=kubernetes
```

修改Kubelet配置文件,设置cgroup驱动为systemd,镜像pause使用阿里云的源.    再启动Kubelet:

```sh
[root@server4-master ~]# cat >/etc/sysconfig/kubelet<<EOF
KUBELET_EXTRA_ARGS="--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.2"
EOF
[root@server4-master ~]# systemctl daemon-reload
[root@server4-master ~]# systemctl enable --now kubelet
```

如果没有配置过docker,需要先修改docker的启动参数采用相同cgroup驱动:

```sh
[root@server7-master ~]# sed -i "s#^ExecStart=/usr/bin/dockerd.*#ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock --exec-opt native.cgroupdriver=systemd#g" /usr/lib/systemd/system/docker.service
[root@server7-master ~]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io
[root@server4-master ~]# systemctl daemon-reload
[root@server7-master ~]# systemctl restart docker
```



## 主节点配置

### 部署keepalived

keepalived是以VRRP(虚拟路由冗余协议)协议为基础,包括一个master和多个backup.master劫持vip对外提供服务.master发送组播,backup节点收不到vrrp包时认为master宕机,此时选出剩余优先级最高的节点作为新master劫持vip.

此处的keeplived的主要作用是为haproxy提供vip(192.168.2.199).在三个haproxy实例之间提供主备,降低当其中一个haproxy失效的时对服务的影响.

先在所有主节点上安装keepalived:

```sh
[root@k8s-250 ~]# yum -y install keepalived psmisc
```

配置文件内容如下.其他节点的配置修改vrrp_instance VI_1中角色state为BACKUP:

```sh
[root@k8s-250 ~]# vi /etc/keepalived/keepalived.conf
global_defs {
    router_id LVS_DEVEL
    script_user root
    enable_script_security
}
vrrp_script check_haproxy {
script "/bin/bash -c 'if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi'"
interval 30
weight -11
}
vrrp_instance VI_1 {
    state MASTER
    interface eth
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.1.253
    }
track_script {
check_haproxy
}
}
```

配置文件中主要配置项有:

- vrrp_script check_haproxy{}: 检测haproxy进程是否存活脚本,interval设置检测间隔为3秒.
- state MASTER: 指定节点名称.
- virtual_router_id 51: 虚拟路由组的id.
- interface ens37: 指定vip绑定网卡名称.
- priority 100: 节点优先级.
- authentication: 节点之间认证密码.
- virtual_ipaddress{}: 虚拟vip的地址.

在所有节点上启动服务:

```sh
[root@k8s-250 ~]# systemctl enable --now keepalived.service
systemctl start keepalived.serviceCreated symlink from /etc/systemd/system/multi-user.target.wants/keepalived.service to /usr/lib/systemd/system/keepalived.service.
```

### 部署HAproxy

haproxy提供高可用性,负载均衡,基于TCP和HTTP的代理,支持数以万记的并发连接.

此处haproxy为apiserver提供反向代理,haproxy将所有请求轮询转发到每个master节点上.

在所有节点上安装并配置haproxy,配置文件内容相同,主要是设置backend kubernetes-apiserver中server的地址列表,负载均衡模式balance默认为轮询的负载算法roundrobin:

```bash
[root@k8s-250 ~]# yum install -y haproxy
[root@k8s-250 ~]# vi /etc/haproxy/haproxy.cfg
#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

#---------------------------------------------------------------------
# kubernetes apiserver frontend which proxys to the backends
#---------------------------------------------------------------------
frontend kubernetes-apiserver
    mode                 tcp
    bind                 *:16443
    option               tcplog
    default_backend      kubernetes-apiserver

#---------------------------------------------------------------------
# round robin balancing between the various backends
#---------------------------------------------------------------------
backend kubernetes-apiserver
    mode        tcp
    balance     roundrobin
    server  k8s-250 192.168.1.250:6443 check
    server  k8s-248 192.168.1.248:6443 check
    server  k8s-249 192.168.1.249:6443 check

#---------------------------------------------------------------------
# collection haproxy statistics message
#---------------------------------------------------------------------
listen stats
    bind                 *:1080
    stats auth           admin:awesomePassword
    stats refresh        5s
    stats realm          HAProxy\ Statistics
    stats uri            /admin?stats
```

启动后观察端口监听状态,16443端口绑定服务监听,1080端口为状态查询前端入口,可以在浏览器访问http://192.168.1.253:1080/admin?stats来查询haproxy运行状态:

```sh
[root@k8s-250 ~]# systemctl enable --now haproxy.service
Created symlink from /etc/systemd/system/multi-user.target.wants/haproxy.service to /usr/lib/systemd/system/haproxy.service.
[root@k8s-250 ~]# systemctl status haproxy
[root@k8s-250 ~]# netstat -ntulp |grep 16443
tcp        0      0 0.0.0.0:16443           0.0.0.0:*               LISTEN      122579/haproxy      
[root@k8s-250 ~]# netstat -ntulp |grep 1080
tcp        0      0 0.0.0.0:1080            0.0.0.0:*               LISTEN      122579/haproxy   
```

使用ip a s查看当前vip绑定的主机,并在停止绑定主机上停止keepalived或haproxy服务后,观察vip的漂移:

```sh
[root@server7-master ~]# ip a s
2: ens37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:f6:ab:84 brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.207/24 brd 192.168.2.255 scope global noprefixroute ens37
       valid_lft forever preferred_lft forever
    inet 192.168.2.199/32 scope global ens37
       valid_lft forever preferred_lft forever
[root@server7-master ~]# systemctl stop haproxy
[root@server8-node1 ~]# ip a s
2: ens37: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 00:0c:29:10:4d:fd brd ff:ff:ff:ff:ff:ff
    inet 192.168.2.208/24 brd 192.168.2.255 scope global noprefixroute ens37
       valid_lft forever preferred_lft forever
    inet 192.168.2.199/32 scope global ens37
       valid_lft forever preferred_lft forever
```

vip会根据配置文件中设置的优先级来选择主机.



## 建立集群

至少需要3个master节点才可组成高可用集群,否则会出现脑裂现象.

### 初始化主节点

可以生成默认kubeadm-master.config配置文件,修改后指定使用配置运行:

```sh
[root@server4-master ~]# kubeadm config print init-defaults > /etc/kubernetes/kubeadm-master.config
[root@server7-master ~]# kubeadm init --config=/etc/kubernetes/kubeadm-master.config --upload-certs
```

或使用指定参数初始化.service-cidr与pod-network-cidr使用的网段10.244.0.0/16对应flannel网络插件.如果是Calico网络插件默认使用pod-network-cidr=192.168.0.0/16:

```sh
[root@server4-master ~]# kubeadm init --apiserver-advertise-address=192.168.2.204 --image-repository=registry.aliyuncs.com/google_containers --kubernetes-version=v1.22.3 --service-cidr=10.96.0.0/12 --pod-network-cidr=10.244.0.0/16
```

主要使用的参数:

- --apiserver-advertise-address: 设置本机的IP地址.
- --image-repository: 设置镜像仓库地址.
- --kubernetes-version: 设置K8s版本
- --service-cidr: 设置服务器通信使用的IP网段.
- --pod-network-cidr: 设置内部的pod节点之间网络可以使用的IP段.
- --upload-certs: 上传证书到kubeadm-certs Secret.
- --control-plane-endpoint: 设置控制节点主机地址.

下面使用声明变量来生成配置

```sh
[root@k8s-250 ~]# export APISERVER_ADVERTISE_ADDRESS=192.168.1.250
[root@k8s-250 ~]# export KUBERNETES_VERSION=v1.22.3
[root@k8s-250 ~]# export SERVICE_CIDR=10.96.0.0/16
[root@k8s-250 ~]# export POD_NETWORK_CIDR=10.244.0.1/16
[root@k8s-250 ~]# export CONTROL_PLANE_ENDPOINT=192.168.1.253:16443
[root@k8s-250 ~]# mkdir -p /nanruan/base/k8s/kubeadm/
[root@k8s-250 ~]# cat <<EOF > /nanruan/base/k8s/kubeadm/kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
localAPIEndpoint:
  advertiseAddress: "${APISERVER_ADVERTISE_ADDRESS}"
  bindPort: 6443
kubernetesVersion: "${KUBERNETES_VERSION}"
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
controlPlaneEndpoint: "${CONTROL_PLANE_ENDPOINT}"
networking:
  serviceSubnet: "${SERVICE_CIDR}"
  podSubnet: "${POD_NETWORK_CIDR}"
  dnsDomain: "cluster.local"
EOF
[root@k8s-250 ~]# kubeadm init --config=/nanruan/base/k8s/kubeadm/kubeadm-config.yaml --upload-certs
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join k8s-250:6443 --token 66e1on.yh84w71x6mauu6em \
        --discovery-token-ca-cert-hash sha256:5849bdad8508feeb3b40e634f7c97f074eb79705d365d097ee75a44374545715 \
        --control-plane --certificate-key 337bb228b52a88c297d72102652834aeef9666b847247b2b17471a78236af909

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-250:6443 --token 66e1on.yh84w71x6mauu6em \
        --discovery-token-ca-cert-hash sha256:5849bdad8508feeb3b40e634f7c97f074eb79705d365d097ee75a44374545715 
```

如果执行失败可以多次执行提前拉取镜像:

```sh
[root@k8s-250 ~]# kubeadm config images pull --image-repository=registry.aliyuncs.com/google_containers
```

### 配置Kubectl

按照提示,将变量声明写入到.bashrc中:

```sh
[root@server4-master ~]# echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >>~/.bashrc
```

或把admin.conf放入$HOME/.kube内,才可使用kubectl命令操作集群::

```sh
[root@k8s-250 ~]# mkdir -p $HOME/.kube && cp -i /etc/kubernetes/admin.conf /root/.kube/config
```

同理可以放到别的用户家目录或其他主机上来赋予集群管理权限.注意需要修改配置文件属主和权限:

```sh
[root@k8s-250 ~]# scp /etc/kubernetes/admin.conf shareuser@192.168.1.248:/home/shareuser/.kube/config
[root@k8s-248 ~]# chown -R shareuser:shareuser /home/shareuser/.kube
[root@k8s-248 ~]# chmod -R 755 /home/shareuser/.kube
```

配置Kubectl命令自动补全:

```sh
[root@k8s-250 ~]# echo "source <(kubectl completion bash)" >>~/.bashrc
[root@k8s-250 ~]# source ~/.bashrc
```

### 部署网络

可以选择Flannel或Calico这种高性能Underlay网络.只能安装一种网络.

- Flannel

  安装Flannel网络方式如下:

  ```sh
  [root@server4-master ~]# kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
  Warning: policy/v1beta1 PodSecurityPolicy is deprecated in v1.21+, unavailable in v1.25+
  podsecuritypolicy.policy/psp.flannel.unprivileged created
  clusterrole.rbac.authorization.k8s.io/flannel created
  clusterrolebinding.rbac.authorization.k8s.io/flannel created
  serviceaccount/flannel created
  configmap/kube-flannel-cfg created
  daemonset.apps/kube-flannel-ds created
  ```

- Calico

  安装Calico网络方式如下:

  ```sh
  [root@server7-master ~]# wget https://kuboard.cn/install-script/calico/calico-3.9.2.yaml
  [root@server7-master ~]# sed -i "s#192\.168\.0\.0/16#${pod_SUBNET}#" calico-3.9.2.yaml
  [root@server7-master ~]# kubectl apply -f calico-3.9.2.yaml
  ```

  官网的安装方式:

  ```sh
  [root@server4-master ~]# kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml
  [root@server4-master ~]# kubectl create -f https://docs.projectcalico.org/manifests/custom-resources.yaml
  installation.operator.tigera.io/default created
  ```

### 删除网络

删除calico网络,先通过配置文件删除相关资源:

```sh
[root@server4-master ~]# kubectl delete -f custom-resources.yaml
[root@server4-master ~]# kubectl delete -f tigera-operator.yaml
```

移除每个节点上的Calico配置文件,然后删除coredns的pod,最后重启kubelet,等待相关pod全部移除,coredns为pending状态即可:

```sh
[root@server5-node1 ~]# rm -rf /etc/cni/net.d/*calico*
[root@server1 ~]# systemctl restart kubelet
[root@server4-master ~]# kubectl delete -n kube-system po coredns-7f6cbbb7b8-vvtxz
[root@server7-master ~]# ifconfig cni0 down
[root@server7-master ~]# ip link delete cni0
```

### 建立完毕

等待网络部署好,所有8个容器部署完毕,主节点便搭建好了:

```sh
[root@k8s-250 ~]# kubectl get pod -n kube-system -o wide --watch
[root@k8s-250 ~]# journalctl -u kubelet -f
```

添加Node标签,NodeRole角色名可以是master(主节点)或worker(从节点).NodeName为主机名.NodeEnv可以是base(基础服务)或app(应用):

```sh
[root@k8s-250 ~]# kubectl label node k8s-250 NodeRole=master
[root@k8s-250 ~]# kubectl label node k8s-250 NodeName=k8s-250
[root@k8s-250 ~]# kubectl label node k8s-250 NodeEnv=base
```





## 加入集群

### 获取参数

加入集群所需3个参数:

- token

  token有效期为1天,失效后可以用token create重建:

  ```sh
  [root@k8s-250 ~]# kubeadm token create
  xjp771.yrjc4oe1thjjt112
  [root@k8s-250 ~]# kubeadm token list
  TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
  xjp771.yrjc4oe1thjjt112   23h         2022-04-21T02:14:35Z   authentication,signing   <none>                                                     system:bootstrappers:kubeadm:default-node-token
  ```

- discovery-token-ca-cert-hash

  获得discovery-token-ca-cert-hash参数:

  ```sh
  [root@k8s-250 ~]# openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
  1499ca908be1d430887b67d4dbfff06a5374cf3d340d3c0ba9db1aa51ad9ddfb
  ```

- certificate-key

  更新证书key:

  ```sh
  [root@k8s-250 ~]# kubeadm init phase upload-certs --upload-certs
  I0420 10:15:58.086168    6701 version.go:255] remote version is much newer: v1.23.5; falling back to: stable-1.22
  [upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
  [upload-certs] Using certificate key:
  937bf7194bd6e346e177f3a64de9ec01e9043828cb263f0ed80bddc359a351e2
  ```

### 加入从节点

加入从节点只需要token和discovery-token-ca-cert-hash参数.指定集群的Control PE或单主节点的IP地址:6443端口,使用kubeadm加入集群:

```sh
[root@k8s-248 ~]# export K8S_PE=192.169.1.253:16443
[root@k8s-248 ~]# kubeadm join ${K8S_PE} --token xjp771.yrjc4oe1thjjt112 \
    --discovery-token-ca-cert-hash sha256:48290e97b917931c237d7edb51b86ca55e36839302edc6d174d3fbb8407c7d2d 
This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

添加Node标签:

```sh
[root@k8s-248 ~]# kubectl label node k8s-248 NodeRole=worker
[root@k8s-248 ~]# kubectl label node k8s-248 NodeName=k8s-248
[root@k8s-248 ~]# kubectl label node k8s-248 NodeEnv=base
```

查看已加入节点:

```sh
[root@server4-master ~]# kubectl get nodes
NAME             STATUS     ROLES                  AGE   VERSION
server4-master   Ready      control-plane,master   31m   v1.22.3
server5-node1    Ready      <none>                 84s   v1.22.3
server6-node2    NotReady   <none>                 19s   v1.22.3
```

通过--all-namespaces参数来查看所有节点容器运行信息,确保所有pod都在运行状态:

```sh
[root@k8s-250 ~]# kubectl get pod --all-namespaces -o wide
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE    IP              NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-7d89d9b6b8-6gkhb          1/1     Running   0          101m   10.244.0.2      k8s-250   <none>           <none>
kube-system   coredns-7d89d9b6b8-qwcmx          1/1     Running   0          101m   10.244.0.3      k8s-250   <none>           <none>
kube-system   etcd-k8s-248                      1/1     Running   0          92m    192.168.1.248   k8s-248   <none>           <none>
kube-system   etcd-k8s-249                      1/1     Running   0          93m    192.168.1.249   k8s-249   <none>           <none>
kube-system   etcd-k8s-250                      1/1     Running   0          101m   192.168.1.250   k8s-250   <none>           <none>
kube-system   kube-apiserver-k8s-248            1/1     Running   0          91m    192.168.1.248   k8s-248   <none>           <none>
kube-system   kube-apiserver-k8s-249            1/1     Running   0          91m    192.168.1.249   k8s-249   <none>           <none>
kube-system   kube-apiserver-k8s-250            1/1     Running   0          91m    192.168.1.250   k8s-250   <none>           <none>
kube-system   kube-controller-manager-k8s-248   1/1     Running   0          90m    192.168.1.248   k8s-248   <none>           <none>
kube-system   kube-controller-manager-k8s-249   1/1     Running   0          90m    192.168.1.249   k8s-249   <none>           <none>
kube-system   kube-controller-manager-k8s-250   1/1     Running   0          90m    192.168.1.250   k8s-250   <none>           <none>
kube-system   kube-flannel-ds-2vtj4             1/1     Running   0          93m    192.168.1.248   k8s-248   <none>           <none>
kube-system   kube-flannel-ds-8t46b             1/1     Running   0          101m   192.168.1.250   k8s-250   <none>           <none>
kube-system   kube-flannel-ds-pj9s9             1/1     Running   0          93m    192.168.1.249   k8s-249   <none>           <none>
kube-system   kube-proxy-8msfm                  1/1     Running   0          93m    192.168.1.249   k8s-249   <none>           <none>
kube-system   kube-proxy-g85tq                  1/1     Running   0          101m   192.168.1.250   k8s-250   <none>           <none>
kube-system   kube-proxy-htszg                  1/1     Running   0          93m    192.168.1.248   k8s-248   <none>           <none>
kube-system   kube-scheduler-k8s-248            1/1     Running   0          90m    192.168.1.248   k8s-248   <none>           <none>
kube-system   kube-scheduler-k8s-249            1/1     Running   0          90m    192.168.1.249   k8s-249   <none>           <none>
kube-system   kube-scheduler-k8s-250            1/1     Running   0          90m    192.168.1.250   k8s-250   <none>           <none>
[root@k8s-250 ~]# kubectl get all --all-namespaces 
```

### 加入主节点

同样是指定集群的Control PE或单主节点的IP地址:6443端口,使用kubeadm加入集群:

```sh
[root@k8s-249 ~]# export K8S_PE=192.168.1.253:16443
[root@k8s-249 ~]# kubeadm join ${K8S_PE} --token 4sz9hf.dwt05n1ohpb772au \
        --discovery-token-ca-cert-hash sha256:864ee6f29ac83d7ec0db3e75c2b54a70ac7622c8c82e52cc71511255febf5b28 \
        --control-plane --certificate-key af928d1032d723ae3f16c7218b07afb41b4d588d746bbeb6ed9c657a460591cd
```

添加集群控制权限:

```sh
[root@k8s-249 ~]# mkdir -p $HOME/.kube && cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
[root@k8s-250 ~]# echo "source <(kubectl completion bash)" >>~/.bashrc
[root@k8s-250 ~]# source ~/.bashrc
```

添加Node标签:

```sh
[root@k8s-249 ~]# kubectl label node k8s-249 NodeRole=master
[root@k8s-249 ~]# kubectl label node k8s-249 NodeName=k8s-249
[root@k8s-249 ~]# kubectl label node k8s-249 NodeEnv=base
```

快速搭建工具另可参考: https://github.com/fanux/sealos



## 移除节点

想从集群中移除节点,先用drain命令驱除节点上运行的pod

```sh
[root@k8s-250 ~]# kubectl drain k8s-249 --ignore-daemonsets
node/k8s-249 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-8rfqm, kube-system/kube-proxy-qnxcw
evicting pod base/zookeeper-2
evicting pod base/zookeeper-0
```

观察到pod都转移之后运行kubectl delete node命令:

```sh
[root@k8s-250 ~]# kubectl delete node k8s-249
node "k8s-249" deleted
```

在要移除的节点上运行kubeadm rest命令:

```sh
[root@k8s-249 ~]# yes | kubeadm reset
[root@k8s-249 ~]# rm -rf /var/lib/cni/ /etc/cni/net.d $HOME/.kube/config /var/lib/etcd
[root@k8s-249 ~]# ifconfig cni0 down
[root@k8s-249 ~]# ip link delete cni0
[root@k8s-249 ~]# ifconfig flannel.1 down
[root@k8s-249 ~]# ip link delete flannel.1
```



## 修改配置

一些常见的配置项目.

### 开放权限

绑定集群管理员角色到所有sa账号:

```sh
[root@k8s-master ~]# kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --group=system:serviceaccounts
clusterrolebinding.rbac.authorization.k8s.io/permissive-binding created
```

删除绑定:

```sh
[root@k8s-master 2]# kubectl delete clusterrolebinding permissive-binding
clusterrolebinding.rbac.authorization.k8s.io "permissive-binding" deleted
```

### 允许主节点部署

移除主节点上的污点来允许部署:

```sh
[root@k8s-250 ~]# kubectl taint nodes --all node-role.kubernetes.io/master=true:NoSchedule-
[root@k8s-250 ~]# kubectl get no -o yaml | grep taint -A 5
[root@k8s-250 ~]# kubectl taint nodes k8s-250 node.kubernetes.io/unreachable-
```

重新添加污点:

```sh
[root@k8s-master pv]# kubectl taint nodes k8s-master node-role.kubernetes.io/master=true:NoSchedule
node/k8s-master tainted
```

### 修改暴露端口范围

修改NodePort端口范围.在kube-apiserver.yaml中加入一行service-node-port-range,服务器会自动更新:

```sh
[root@k8s-250 ~]# sed -i '40a\    - --service-node-port-range=1-65535' /etc/kubernetes/manifests/kube-apiserver.yaml
[root@k8s-250 ~]# cat /etc/kubernetes/manifests/kube-apiserver.yaml
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    - --service-node-port-range=1-65535
    image: registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.22.3
    imagePullPolicy: IfNotPresent
```

### 恢复健康检查

注释掉scheduler和controller-manager配置中的端口绑定,免得健康检查出错:

```sh
[root@k8s-250 ~]# sed -i "s/    - --port=0/#    - --port=0/g" /etc/kubernetes/manifests/kube-controller-manager.yaml
[root@k8s-250 ~]# sed -i "s/    - --port=0/#    - --port=0/g" /etc/kubernetes/manifests/kube-scheduler.yaml
[root@k8s-250 ~]# systemctl restart kubelet.service
[root@k8s-250 ~]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE                         ERROR
scheduler            Healthy   ok                              
controller-manager   Healthy   ok                              
etcd-0               Healthy   {"health":"true","reason":""}   
```



## 插件安装

主要是各种可以通过浏览器来可视化管理k8s的项目.

### Dashboard

K8s的WebUI需要额外部署:

```sh
[root@k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.4/aio/deploy/recommended.yaml
```

修改node为NodePort模式:

```sh
[root@k8s-master ~]# kubectl patch svc -n kubernetes-dashboard kubernetes-dashboard -p '{"spec":{"type":"NodePort"}}'
service/kubernetes-dashboard patched
```

查看NodePort端口:

```sh
[root@server4-master ~]# kubectl get svc --all-namespaces
NAMESPACE              NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
kubernetes-dashboard   kubernetes-dashboard              NodePort    10.107.244.106   <none>        443:31163/TCP            21s
```

还需要创建token才可以通过浏览器访问31163端口:

```sh
[root@k8s-master 10]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
serviceaccount/dashboard-admin created
[root@k8s-master 10]# kubectl create clusterrolebinding dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
clusterrolebinding.rbac.authorization.k8s.io/dashboard-cluster-admin created
[root@k8s-master 10]# kubectl describe secret -n kubernetes-dashboard dashboard-admin-token
Name:         dashboard-admin-token-nrm6q
Namespace:    kubernetes-dashboard
Labels:       <none>
Annotations:  kubernetes.io/service-account.name: dashboard-admin
              kubernetes.io/service-account.uid: 5f0cc597-e403-47af-b3b1-25de06c82723

Type:  kubernetes.io/service-account-token

Data
====
namespace:  20 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbnJtNnEiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNWYwY2M1OTctZTQwMy00N2FmLWIzYjEtMjVkZTA2YzgyNzIzIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.uBjaS6PqTJTJvK9B9M8yNp--BpAdLzOpzFiqfgkd1GIPI3haOSQMmLufE-XIkbPhz4NLmmyJ5z4Jhini7WJgKTn8p9CR2M_yhlaPhOHaZl3WqjEAp2AwU98PFIjlNgbRv7U1vEt1HEL0mJ6HPLo8fqHa4AziwlwYm_uTqmk5sVm9V0YisRs-21V5IohLfpEWIBfU2ddpdqzZV3qkgEbVYGo3GQ3CBKjDilJh-DtMvejpJRUSetKxL97VLmYXwHLvrkqZJzvccCBkAreXKbXpJVTzrs9ya67RTqalw_tWxFlB--ASLRcmoEMxdfJ406G-YF5NEDK20jIvK6HitSurqQ
ca.crt:     1025 bytes
```

### Rook

Rook是一个基于Ceph的K8s储存插件,同样可以使用容器直接部署:

```sh
[root@server4-master ~]# git clone https://github.com/rook/rook.git
[root@server4-master ~]# cd rook/cluster/examples/kubernetes/ceph
```

但由于k8s.gcr.io访问不了,所以需要修改配置文件:

```sh
[root@server4-master ceph]# kubectl delete -f crds.yaml -f common.yaml -f operator.yaml -f cluster.yaml
[root@server4-master ceph]# vi operator.yaml
ROOK_CSI_CEPH_IMAGE: "quay.io/cephcsi/cephcsi:v3.1.2"
ROOK_CSI_REGISTRAR_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-node-driver-registrar:v2.0.1"
ROOK_CSI_RESIZER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-resizer:v1.0.0"
ROOK_CSI_PROVISIONER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-provisioner:v2.0.0"
ROOK_CSI_SNAPSHOTTER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-snapshotter:v3.0.0"
ROOK_CSI_ATTACHER_IMAGE: "registry.cn-beijing.aliyuncs.com/dotbalo/csi-attacher:v3.0.0"
[root@server4-master ceph]# kubectl create -f crds.yaml -f common.yaml -f operator.yaml
[root@server4-master ceph]# kubectl create -f cluster.yaml
```

### 其他项目

其他第三方管理工具还有[Kuboard](https://kuboard.cn/)和[KubeSphere](https://kubesphere.io/)



## 集群升级

整个集群升级需要分三步进行: 

- 第一步主节点升级kubeadm和kubectl.
- 第二步主节点升级kubelet.
- 第三步从节点升级kubelet.

主节点升级kubeadm和kubectl对集群运行没有影响:

```sh
[root@k8s-250 ~]# yum list | grep 'kubeadm\|kubectl\|kubelet'
kubeadm.x86_64                           1.22.3-0                      @kubernetes
kubectl.x86_64                           1.22.3-0                      @kubernetes
kubelet.x86_64                           1.22.3-0                      @kubernetes
kubeadm.x86_64                           1.23.5-0                      kubernetes
kubectl.x86_64                           1.23.5-0                      kubernetes
kubelet.x86_64                           1.23.5-0                      kubernetes
[root@k8s-250 ~]# yum -y install kubeadm-1.23.5-0 kubectl-1.23.5-0
[root@k8s-250 ~]# kubeadm upgrade plan
[root@k8s-250 ~]# kubeadm upgrade apply v1.23.5-0 --config /nanruan/base/k8s/kubeadm/kubeadm-config.yaml
```

主节点和从节点升级kubelet步骤一样,先禁用调度,升级并重启kubelet,最后重设允许调度:

```sh
[root@k8s-250 ~]# kubectl drain k8s-250 --ignore-daemonsets
[root@k8s-250 ~]# yum -y install kubelet-1.23.5-0
[root@k8s-250 ~]# systemctl daemon-reload
[root@k8s-250 ~]# systemctl restart kubelet
[root@k8s-250 ~]# systemctl status kubelet
[root@k8s-250 ~]# kubectl uncordon k8s-250
[root@k8s-250 ~]# kubectl get nodes
[root@k8s-250 ~]# kubectl get all --all-namespaces
```



## 故障处理

一些集群组建时遇到的问题.

### 没有pod-network-cidr

安装Flannel报错提示没有分配pod cidr.这是由于使用kubeadm初始化时没有指定--pod-network-cidr参数,或者kube-flannel.yml中的"Network": "10.244.0.0/16"和--pod-network-cidr参数不一样所致:

```sh
[root@server4-master ~]# kubectl logs -n kube-system kube-flannel-ds-7xp24 
I1103 04:42:22.796211       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false
E1103 04:42:22.797028       1 main.go:325] Error registering network: failed to acquire lease: node "server4-master" pod cidr not assigned
I1103 04:42:22.797248       1 main.go:439] Stopping shutdownHandler...
```

修改配置文件kube-controller-manager.yaml,添加- --allocate-node-cidrs=true和- --cluster-cidr=10.244.0.0/16:

```sh
[root@server4-master ~]# vi /etc/kubernetes/manifests/kube-controller-manager.yaml
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --cluster-cidr=10.244.0.0/16
```

删除错误的flannel容器,重建之后就能使用正确的配置了:

```sh
[root@server4-master ~]# kubectl delete pod -n kube-system kube-flannel-*
[root@server4-master ~]# kubectl cluster-info dump | grep -m 1 cluster-cidr
                            "--cluster-cidr=10.244.0.0/16",
```

### 没有controlPlaneEndpoint

在第二个主节点加入一个已存在的单节点集群过程中,preflight阶段会报错没有controlPlaneEndpoint.这也是在初始化时没指定--Control-Plane-Endpoint参数造成:

```sh
[root@server1 ~]# kubeadm join 192.168.2.204:6443 --token nre8hl.4awo2mnyxr5l0tsu      --discovery-token-ca-cert-hash sha256:8055ad40e280bc4b48b0c3b4daea9aa3f515d3b2fea5fb3ae3fcad398c325a52      --control-plane --certificate-key 5ac22cea4cfc19753cd289c96b968957554b4e4204fe47fc3f56a53dded96716
error execution phase preflight: 
One or more conditions for hosting a new control plane instance is not satisfied.

unable to add a new control plane instance a cluster that doesn't have a stable controlPlaneEndpoint address

Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.
```

使用kubectl edit命令编辑kubeadm-config配置文件,添加controlPlaneEndpoint地址为vip或运行中的主节点IP:

```sh
[root@server4-master ~]# kubectl edit cm kubeadm-config -n kube-system
    imageRepository: registry.aliyuncs.com/google_containers
    kind: ClusterConfiguration
    controlPlaneEndpoint: 192.168.2.204:6443
    kubernetesVersion: v1.22.3
    networking:
      dnsDomain: cluster.local
      podSubnet: 10.244.0.0/16
```

### Token过期

加入时报错x509: certificate has expired or is not yet valid,可以重新生成token:

```sh
[root@k8s-master ~]# kubeadm token create
87668a.lfvdqsr3l5ijlode
[root@k8s-master ~]# kubeadm token list
TOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS
1yrznt.lzj1kl0qecosl4es   23h       2019-07-21T13:14:33+08:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token
87668a.lfvdqsr3l5ijlode   23h       2019-07-21T13:24:28+08:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token
[root@server7-master ~]# kubeadm init phase upload-certs --upload-certs
```

在客户端同步时间,再次加入成功:

```sh
[root@k8s-node1 ~]# ntpdate cn.pool.ntp.org
20 Jul 13:25:59 ntpdate[118642]: step time server 199.182.204.197 offset 137559.272548 sec
```

### 证书过期

可以用下面命令来查看证书有效期:

```sh
[root@server4-master ~]# kubeadm certs check-expiration
[check-expiration] Reading configuration from the cluster...
[check-expiration] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

CERTIFICATE                EXPIRES                  RESIDUAL TIME   CERTIFICATE AUTHORITY   EXTERNALLY MANAGED
admin.conf                 Nov 03, 2022 05:36 UTC   217d                                    no      
apiserver                  Nov 03, 2022 05:36 UTC   217d            ca                      no      
apiserver-etcd-client      Nov 03, 2022 05:36 UTC   217d            etcd-ca                 no      
apiserver-kubelet-client   Nov 03, 2022 05:36 UTC   217d            ca                      no      
controller-manager.conf    Nov 03, 2022 05:36 UTC   217d                                    no      
etcd-healthcheck-client    Nov 03, 2022 05:36 UTC   217d            etcd-ca                 no      
etcd-peer                  Nov 03, 2022 05:36 UTC   217d            etcd-ca                 no      
etcd-server                Nov 03, 2022 05:36 UTC   217d            etcd-ca                 no      
front-proxy-client         Nov 03, 2022 05:36 UTC   217d            front-proxy-ca          no      
scheduler.conf             Nov 03, 2022 05:36 UTC   217d                                    no      

CERTIFICATE AUTHORITY   EXPIRES                  RESIDUAL TIME   EXTERNALLY MANAGED
ca                      Nov 01, 2031 05:36 UTC   9y              no      
etcd-ca                 Nov 01, 2031 05:36 UTC   9y              no      
front-proxy-ca          Nov 01, 2031 05:36 UTC   9y              no   
```

重新生成所有证书:

```sh
[root@server4-master ~]# kubeadm certs renew all
[renew] Reading configuration from the cluster...
[renew] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'

certificate embedded in the kubeconfig file for the admin to use and for kubeadm itself renewed
certificate for serving the Kubernetes API renewed
certificate the apiserver uses to access etcd renewed
certificate for the API server to connect to kubelet renewed
certificate embedded in the kubeconfig file for the controller manager to use renewed
certificate for liveness probes to healthcheck etcd renewed
certificate for etcd nodes to communicate with each other renewed
certificate for serving etcd renewed
certificate for the front proxy client renewed
certificate embedded in the kubeconfig file for the scheduler manager to use renewed

Done renewing certificates. You must restart the kube-apiserver, kube-controller-manager, kube-scheduler and etcd, so that they can use the new certificates.
```

按照提示分别重启所有主节点上组件容器:

```sh
[root@server4-master ~]# docker ps | grep -E 'k8s_kube-apiserver|k8s_kube-controller-manager|k8s_kube-scheduler|k8s_etcd_etcd' | awk -F ' ' '{print $1}' |xargs docker restart
```

### 证书不符

如果加入时报错x509: certificate is valid for 10.96.0.1, 192.168.2.204, not 192.168.2.199提示签证的IP地址不符,这在更新了节点apiserver-advertise-address或controlPlaneEndpoint时很常见.解决办法是删除当前集群下apiserver的cert和key并重新生成:

```sh
[root@server4 ~]# rm -f /etc/kubernetes/pki/apiserver.*
```

设置原先主机IP地址192.168.2.204和现在的虚拟IP地址192.168.2.199:

```sh
[root@server4 ~]# kubeadm init phase certs apiserver --apiserver-advertise-address 192.168.2.204 --apiserver-cert-extra-sans 192.168.2.199
[root@server4 ~]# kubeadm alpha certs renew admin.conf
Kubeadm experimental sub-commands
```

重启api server,查看证书:

```sh
[root@server4 ~]# kubectl -n kube-system delete pod kube-apiserver-server4 kube-apiserver-server6
pod "kube-apiserver-server4" deleted
pod "kube-apiserver-server6" deleted
[root@server4 ~]# openssl x509 -in /etc/kubernetes/pki/apiserver.crt -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 2687592491658220129 (0x254c3f21b64b9261)
    Signature Algorithm: sha256WithRSAEncryption
        Issuer: CN=kubernetes
        Validity
            Not Before: Apr  4 16:21:14 2022 GMT
            Not After : Apr 19 13:48:19 2023 GMT
```

将新的admin.conf拷贝到其他要访问集群的主节点:

```sh
[root@server4 ~]# scp /etc/kubernetes/admin.conf root@192.168.2.206:./.kube
```

也可以加入新主节点了.

### cni0地址错误

在重建k8s集群后,由于节点中残留虚拟网卡没有删除,导致创建pod失败,提示已经存在cni0地址冲突:

```sh
[root@server7-master ~]# kubectl describe po kubia-74967b5695-ftk77
  Warning  FailedCreatePodSandBox  56s (x4 over 59s)   kubelet            (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container "18dd9bc5076666ed605b2d3331864859454878d7e1f9fae2160f1be71c59c48f" network for pod "kubia-74967b5695-ftk77": networkPlugin cni failed to set up pod "kubia-74967b5695-ftk77_default" network: failed to delegate add: failed to set bridge addr: "cni0" already has an IP address different from 10.100.4.1/24
[root@server6 ~]# ip a
5: cni0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 92:1d:b1:d9:ba:19 brd ff:ff:ff:ff:ff:ff
    inet 10.100.9.1/24 brd 10.100.9.255 scope global cni0
       valid_lft forever preferred_lft forever
```

解决办法是手动删除网卡,让它自动重建:

```sh
[root@server6 ~]# ifconfig cni0 down
[root@server6 ~]# ip link delete cni0
[root@server6 ~]# ip a
926: cni0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UP group default qlen 1000
    link/ether 92:6a:c8:8b:2c:5b brd ff:ff:ff:ff:ff:ff
    inet 10.100.4.1/24 brd 10.100.4.255 scope global cni0
       valid_lft forever preferred_lft forever
```

### etcd错误

etcd中存在已被删除的节点记录,节点重新加入时检查报错etcd cluster is not healthy: failed to dial endpoint.这时需要手动维护etcd数据库.

进入到节点中etcd容器里查询etcd集群成员列表,删除错误成员后集群便恢复正常:

```sh
[root@server6 ~]# kubectl -n kube-system exec -it etcd-server6 -- sh
sh-5.0# export ETCDCTL_API=3
sh-5.0# alias etcdctl='etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key'
sh-5.0# etcdctl member list
2e1d3dc8363ee756, started, server5, https://192.168.2.205:2380, https://192.168.2.205:2379, false
4b8b623fba4c1f57, started, server6, https://192.168.2.206:2380, https://192.168.2.206:2379, false
ad703513a2c4e54a, started, server4, https://192.168.2.204:2380, https://192.168.2.204:2379, false
sh-5.0# etcdctl member remove 63bfe05c4646fb08
```



## 阿里云部署

在阿里云上部署K8s时ecs不支持自建vip,指定slb的vip为高可用地址时初始化master会失败.这是由于4层SLB不支持其调度的后端服务器访问其vip,即服务器不能又当服务端又做客户端.

解决办法是在hosts文件里解析个域名作为controlPlaneEndpoint地址,主节点的hosts文件把127.0.0.1指向此域名,node的hosts正常解析此域名为slb地址即可.

参考网址:

https://www.cnblogs.com/gmmy/p/12372805.html

https://www.cnblogs.com/noah-luo/p/13218837.html

https://zhuanlan.zhihu.com/p/107703112

https://blog.51cto.com/caiyuanji/2405434

https://blog.csdn.net/chenleiking/article/details/84841394

https://jimmysong.io/kubernetes-handbook/

https://www.jianshu.com/p/ba94db1c3599

https://blog.csdn.net/weixin_44334279/article/details/119355653

https://www.jianshu.com/p/d645bbe8e621

https://www.kubernetes.org.cn/7033.html
