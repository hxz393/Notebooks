# 高级调度

## 禁止调度

有时需要手动标记节点为不可调度来做对节点做维护,可以用cordon命令来标记节点不再接收新的pod请求:

```sh
[root@server4-master ~]# kubectl cordon server5-node1
node/server5-node1 cordoned
```

还可以通过drain命令在停止调度后,把其上的pod疏散到其他节点:

```sh
[root@server4-master ~]# kubectl drain server5-node1 
node/server5-node1 already cordoned
DEPRECATED WARNING: Aborting the drain command in a list of nodes will be deprecated in v1.23.
The new behavior will make the drain command go through all nodes even if one or more nodes failed during the drain.
For now, users can try such experience via: --ignore-errors
error: unable to drain node "server5-node1", aborting command...

There are pending nodes to be drained:
 server5-node1
cannot delete Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet (use --force to override): bar/test, default/host-pid, default/pod-host-network, default/pod-privileged, foo/test
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): calico-system/calico-node-2cp86, kube-system/kube-proxy-fnvff
cannot delete Pods with local storage (use --delete-emptydir-data to override): default/pod-readonly
```

按照提示可以增加一些参数如--force to override,--ignore-daemonsets to ignore等来忽略警告.

想要解除不可调度状态,可以用uncordon命令:

```sh
[root@server4-master ~]# kubectl uncordon server5-node1
node/server5-node1 uncordoned
```



## 污点和容忍度

在用kubeadm建立集群时,主节点会自动设置污点,不允许被部署.可以通过describe查看污点信息:

```sh
[root@server4-master ~]# kubectl describe node server4-master 
Taints:             node-role.kubernetes.io/master=true:NoSchedule
```

其中Taints选项包含key,value以及一个effect.主节点上污点信息键为node-role.kubernetes.io/master,值为true,effect为NoSchedule.这个污点将阻止pod调度到这个节点上,除非有pod能够容忍.通常容忍这个污点的pod都是系统级别pod.例如kube-proxy:

```sh
[root@server4-master ~]# kubectl describe po kube-proxy-wkgjf -n kube-system
Tolerations:                 op=Exists
                             node.kubernetes.io/disk-pressure:NoSchedule op=Exists
                             node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/network-unavailable:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists
                             node.kubernetes.io/pid-pressure:NoSchedule op=Exists
                             node.kubernetes.io/unreachable:NoExecute op=Exists
                             node.kubernetes.io/unschedulable:NoSchedule op=Exists
```



## 污点效果

每个污点都关联一个效果,效果有三种:

- NoSchedule

  如果Pod没有容忍这些污点,pod则不能被调度到包含这些污点的节点上.

- PreferNoSchedule

  表示尽量阻止pod被调度到这个节点上.如果没有其他节点可用时才能被调度.

- NoExecute

  会影响正在节点上运行的pod,如果节点上添加了NoExecute污点,那些节点上运行着的pod会从这个节点去除.



## 添加污点

平时可以在生产环境的节点上添加污点来拒绝部署:

```sh
[root@server4-master ~]# kubectl taint node server4-master node-type=prod:NoSchedule
node/server4-master tainted
[root@server4-master ~]# kubectl describe node server4-master 
Taints:             node-type=prod:NoSchedule
```

节点添加了一个taint,其中键为node-type,值为prod,效果为NoSchedule.



## 污点容忍度

如果要向有污点的节点部署pod,可以在spec.template.spec.tolerations中添加污点容忍度:

```sh
[root@server4-master ~]# vi dp.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
      - name: nodejs
        image: luksa/kubia
        ports:
        - name: http
          containerPort: 8080
        resources:
          requests:
            cpu: 100m
      tolerations:
      - key: node-type
        operator: Equal
        value: prod
        effect: NoSchedule
```

可以灵活地配置节点失效(unready或unreachable状态)中,pod重新调度前等待多长时间:

```sh
      tolerations:
      - key: node.alphakubernetes.io/notReady
        operator: Exists
        tolerationSeconds: 30
        effect: NoExecute
      - key: node.alphakubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 300
        effect: NoExecute
```

当没有定义这两个容忍度时,它们会自动添加到pod上.初始时间为300秒.可以减少值来减低等待时间.



## 删除污点

在增加污点的命令最后加上减号-即可删除污点:

```sh
[root@server4-master ~]# kubectl taint node server5-node1 node-type=prod:NoSchedule-
node/server5-node1 untainted
```



## 节点亲缘性

节点亲缘性(node affinity)允许k8s将pod只调度到某个几点子集上面.

节点亲缘性规则用以取代节点选择器其写法如下:

```sh
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: gpu
              operator: in
              values:
              - "true"
```

其中requiredDuringSchedulingIgnoredDuringExecution用于表示强制性规则,不影响已经在节点上运行的pod.规则为pod只会调度到包含gpu=true标签的节点上.

可以通过preferredDuringSchedulingIgnoredDuringExecution设置软性规则,来指定多个节点,并调整优先度:

```sh
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 80
        preference:
            matchExpressions:
            - key: gpu
              operator: In
              values:
              - "true"
      - weight: 20
        preference:
            matchExpressions:
            - key: zone
              operator: In
              values:
              - "cn"
```

其中节点亲缘性优先级设置了pod被优先调度到标签为gpu=true和zone=cn节点上,其次是拥有gpu=true但zone值不为cn的节点,再次是zone=cn节点,最后则是优先级最低的其他节点.

对node1打上gpu=true和zone=cn标签:

```sh
[root@server4-master ~]# kubectl label node server5-node1 gpu=true
node/server5-node1 labeled
[root@server4-master ~]# kubectl label node server5-node1 zone=cn
node/server5-node1 labeled
```

使用Deploy发布有5个副本的应用,查看pod分布:

```sh
[root@server4-master ~]# kubectl create -f deploy.label.yaml 
deployment.apps/kubia created
[root@server4-master ~]# kubectl get po -o wide
NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE         kubia-6f84b588d-8b6dn   1/1     Running   0          9s    10.244.191.247   server6-node2 
kubia-6f84b588d-kbrks   1/1     Running   0          9s    10.244.191.250   server5-node1 
kubia-6f84b588d-kf6p2   1/1     Running   0          9s    10.244.191.251   server5-node1 
kubia-6f84b588d-pbf7p   1/1     Running   0          9s    10.244.191.248   server5-node1 
kubia-6f84b588d-sfqv4   1/1     Running   0          9s    10.244.191.249   server5-node1 
```

可以看到5个pod中有4个被部署到了node1,另外1个部署到node2中.这是Selector SpreadPriority函数的作用,它确保属于同一RS或Service的pod分布到不同节点,避免单节点失败导致服务不可用.



## pod亲缘性

如果是协同工作的2个pod,可以设置pod亲缘性将其尽可能部署到同一节点:

```sh
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            app: backend
```

此部署将创建包含强制性要求的pod,其中要求opd将被调度到和其他包含app=backend标签的pod所在的相同节点上(通过topologyKey字段指定).



## 非亲缘性

除了能让调度器对pod进行协同部署,还能让pod之间远离彼此.通过podAntiAffinity字段来配置,这将导致调度器永远不会选择那些有包含podAntiAffinity匹配标签的pod所在节点.

通常用于分离同样耗费大量系统资源的pod,或者将pod分在不同可用区,提升服务可用性.

